{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an OpenVINO™ Inference Engine\n",
    "In this section, we will create an inference engine wrapper, a class that will get image data as input, analyze it, and return the analysis result. We will be using Intel OpenVINO™ runtime for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Global Variables\n",
    "First, read the previously stored variables. We need the name of the directory that will be used to store our ML solution files. If this directory does not exist, we will create a directory with a specified directory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceserver\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from env_variables import *\n",
    "\n",
    "import os\n",
    "print(isSolutionPath)\n",
    "if not os.path.exists(isSolutionPath):\n",
    "    os.mkdir(isSolutionPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the OpenVINO™ ML model\n",
    "Next, we will download Intel's OpenVINO™ Intermediate Representation models (BIN + XML files) for object detection. We will download multiple models, each with different capabilities. For more details about the capabilities of these models, please refer to the link provided below.  \n",
    " \n",
    "To add different models to this sample, please click on a model in the [ModelZoo page](https://github.com/opencv/open_model_zoo/blob/master/models/intel/index.md) to view its performance details and then update the `modelNames` list below.  \n",
    "\n",
    "In this sample, we will be downloading the following models:\n",
    "\n",
    "* person detectors\n",
    "    * person-detection-retail-0002 \n",
    "    * person-detection-retail-0013\n",
    "    * pedestrian-detection-adas-0002\n",
    "    * pedestrian-detection-adas-binary-0001\n",
    "* person + vehicle detectors\n",
    "    * pedestrian-and-vehicle-detector-adas-0001\n",
    "    * person-vehicle-bike-detection-crossroad-0078\n",
    "    * person-vehicle-bike-detection-crossroad-1016\n",
    "* vehicle detectors\n",
    "    * vehicle-detection-adas-0002\n",
    "    * vehicle-detection-adas-binary-0001\n",
    "    * vehicle-license-plate-detection-barrier-010\n",
    "\n",
    "The following code cell will download these models from ModelZoo and store them in our ML solution directory.\n",
    "\n",
    "> <span>[!NOTE]</span>\n",
    "> The model download URLs frequently change, so if any of the code below fails, please update the source URLs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: person-detection-retail-0002\n",
      "./inferenceserver/models/person-detection-retail-0002 already exists here, so not downloading again.\n",
      "Downloading: person-detection-retail-0013\n",
      "./inferenceserver/models/person-detection-retail-0013 already exists here, so not downloading again.\n",
      "Downloading: pedestrian-detection-adas-0002\n",
      "./inferenceserver/models/pedestrian-detection-adas-0002 already exists here, so not downloading again.\n",
      "Downloading: pedestrian-detection-adas-binary-0001\n",
      "./inferenceserver/models/pedestrian-detection-adas-binary-0001 already exists here, so not downloading again.\n",
      "Downloading: pedestrian-and-vehicle-detector-adas-0001\n",
      "./inferenceserver/models/pedestrian-and-vehicle-detector-adas-0001 already exists here, so not downloading again.\n",
      "Downloading: person-vehicle-bike-detection-crossroad-0078\n",
      "./inferenceserver/models/person-vehicle-bike-detection-crossroad-0078 already exists here, so not downloading again.\n",
      "Downloading: person-vehicle-bike-detection-crossroad-1016\n",
      "./inferenceserver/models/person-vehicle-bike-detection-crossroad-1016 already exists here, so not downloading again.\n",
      "Downloading: vehicle-detection-adas-0002\n",
      "./inferenceserver/models/vehicle-detection-adas-0002 already exists here, so not downloading again.\n",
      "Downloading: vehicle-detection-adas-binary-0001\n",
      "./inferenceserver/models/vehicle-detection-adas-binary-0001 already exists here, so not downloading again.\n",
      "Downloading: vehicle-license-plate-detection-barrier-0106\n",
      "./inferenceserver/models/vehicle-license-plate-detection-barrier-0106 already exists here, so not downloading again.\n",
      "Downloading: text-recognition-0012\n",
      "./inferenceserver/models/text-recognition-0012 already exists here, so not downloading again.\n",
      "Downloading: license-plate-recognition-barrier-0001\n",
      "./inferenceserver/models/license-plate-recognition-barrier-0001 already exists here, so not downloading again.\n"
     ]
    }
   ],
   "source": [
    "# Download the model files\n",
    "import os\n",
    "import yaml\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def getModel(modelName, isSolutionPath):\n",
    "    isSolutionModelFilePath = os.path.join(\".\", isSolutionPath, \"models\", modelName)\n",
    "\n",
    "    if not os.path.exists(isSolutionModelFilePath):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/open_model_zoo/master/models/intel/{0}/model.yml\".format(modelName)\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        yamlFileContent = resp.read()\n",
    "\n",
    "        yamlContent = yaml.load_all(yamlFileContent)\n",
    "        for c in yamlContent:\n",
    "            for k1, v1 in c.items():\n",
    "                if k1 == \"files\":\n",
    "                    for k2, v2 in enumerate(v1):\n",
    "                        for k in v2:\n",
    "                            if k == \"name\":\n",
    "                                modelPathName = v2[k] \n",
    "                            if k == \"source\":\n",
    "                                modelSourceURL = v2[k]\n",
    "                                # Download the model\n",
    "                                pathDownload = os.path.join(isSolutionModelFilePath, modelPathName)\n",
    "                                headTail = os.path.split(pathDownload) \n",
    "                                os.makedirs(headTail[0], exist_ok=True)\n",
    "                                res = urllib.request.urlretrieve(modelSourceURL, pathDownload)\n",
    "                                print(\"{} downloaded\".format(pathDownload))\n",
    "                                break\n",
    "                    break\n",
    "    else:\n",
    "        print(\"{} already exists here, so not downloading again.\".format(isSolutionModelFilePath))\n",
    "\n",
    "\n",
    "# see full list of models at Model Zoo...\n",
    "modelNames = [  # person detectors\n",
    "                \"person-detection-retail-0002\", \n",
    "                \"person-detection-retail-0013\", \n",
    "                \"pedestrian-detection-adas-0002\", \n",
    "                \"pedestrian-detection-adas-binary-0001\", \n",
    "                # person + vehicle detectors\n",
    "                \"pedestrian-and-vehicle-detector-adas-0001\",\n",
    "                \"person-vehicle-bike-detection-crossroad-0078\",\n",
    "                \"person-vehicle-bike-detection-crossroad-1016\",\n",
    "                # vehicle detectors\n",
    "                \"vehicle-detection-adas-0002\",\n",
    "                \"vehicle-detection-adas-binary-0001\",\n",
    "                \"vehicle-license-plate-detection-barrier-0106\",\n",
    "                # license plate recognition\n",
    "                \"text-recognition-0012\",\n",
    "                \"license-plate-recognition-barrier-0001\"]\n",
    "\n",
    "for modelName in modelNames:\n",
    "    print(\"Downloading: {0}\".format(modelName))\n",
    "    getModel(modelName, isSolutionPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cell above returns an issue related to the `import yaml` command, you can download the package by running `pip3 install pyyaml` based on your version of Pip. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Engine Wrapper\n",
    "Next, we will create a class that with different properties and methods to help scoring and analysing data from an image. This class will also help us specify analytics compute targets, such as CPU, VPU, FPGA, and debugging features.\n",
    "\n",
    "The AnalyticsAPI class below has a parameter named `targetDev`, which can be used to specify what type of Intel® hardware acceleration should be used. You can set this parameter in the next section, when you create the container image. The four options are \"CPU\" (CPU acceleration), \"MYRIAD\" (VPU acceleration), \"GPU\" (GPU acceleration), and \"FPGA\" (FPGA acceleration). Intel® also has different plugins and extensions for your choice of hardware acceleration.\n",
    "\n",
    "Read the following documentation to more about [Intel® accelerated hardware](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/hardware.html), [plugins for accelerated hardware](https://docs.openvinotoolkit.org/2018_R5/_docs_IE_DG_supported_plugins_Supported_Devices.html), and [developing with the OpenVINO™ toolkit](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/score.py\n",
    "\n",
    "import sys\n",
    "import linecache\n",
    "import threading\n",
    "from collections import OrderedDict\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "import timeit as t\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def PrintGetExceptionDetails():\n",
    "    exType, exValue, exTraceback = sys.exc_info()\n",
    "\n",
    "    tbFrame = exTraceback.tb_frame\n",
    "    lineNo = exTraceback.tb_lineno\n",
    "    fileName = tbFrame.f_code.co_filename\n",
    "\n",
    "    linecache.checkcache(fileName)\n",
    "    line = linecache.getline(fileName, lineNo, tbFrame.f_globals)\n",
    "\n",
    "    exMessage = '[IS] Exception:\\n\\tFile name: {0}\\n\\tLine number: {1}\\n\\tLine: {2}\\n\\tValue: {3}'.format(\n",
    "        fileName, lineNo, line.strip(), exValue)\n",
    "\n",
    "    logging.info(exMessage)\n",
    "\n",
    "\n",
    "class MLModel:\n",
    "\n",
    "    # targetDev can take on the values of \"CPU\", \"MYRIAD\", \"GPU\", \"FPGA\"\n",
    "    def __init__(self, modelName, modelPrecision, targetDev, pluginPath=None, cpuExtensions=None):\n",
    "        try:\n",
    "            self.initialized = False\n",
    "            self.modelPath = \"./models\"\n",
    "            self.modelName = modelName\n",
    "            self.modelPrecision = modelPrecision\n",
    "            self.targetDev = targetDev\n",
    "            self.pluginPath = pluginPath\n",
    "            self.cpuExtensions = cpuExtensions\n",
    "            self._lock = threading.Lock()\n",
    "            self.initEngine()\n",
    "\n",
    "        except:\n",
    "            PrintGetExceptionDetails()\n",
    "\n",
    "    def initEngine(self):\n",
    "        try:\n",
    "            with self._lock:\n",
    "                self.modelXMLFileName = os.path.join(\n",
    "                    self.modelPath, self.modelName, self.modelPrecision, self.modelName + \".xml\")\n",
    "                self.modelBINFileName = os.path.join(\n",
    "                    self.modelPath, self.modelName, self.modelPrecision, self.modelName + \".bin\")\n",
    "\n",
    "                # Initiate targetDev for hardware acceleration. Default is CPU acceleration\n",
    "                self.iePlugin = IEPlugin(\n",
    "                    device=self.targetDev, plugin_dirs=self.pluginPath)\n",
    "                if self.cpuExtensions and 'CPU' in self.targetDev:\n",
    "                    self.iePlugin.add_cpu_extension(self.cpuExtensions)\n",
    "\n",
    "                ieNet = IENetwork(model=self.modelXMLFileName,\n",
    "                                  weights=self.modelBINFileName)\n",
    "\n",
    "                assert len(ieNet.inputs.keys()\n",
    "                           ) == 1, \"Only single input topologies supported!\"\n",
    "                assert len(\n",
    "                    ieNet.outputs) == 1, \"Only single output topologies supported!\"\n",
    "                self.inputBlob = next(iter(ieNet.inputs))\n",
    "                self.outBlob = next(iter(ieNet.outputs))\n",
    "                self.ieExecNet = self.iePlugin.load(\n",
    "                    network=ieNet, num_requests=2)\n",
    "\n",
    "                # Read and pre-process input image\n",
    "                # n, c, h, w\n",
    "                self.ieNetShape = ieNet.inputs[self.inputBlob].shape\n",
    "\n",
    "        except:\n",
    "            PrintGetExceptionDetails()\n",
    "\n",
    "    def preprocess(self, cvImage):\n",
    "        try:\n",
    "            ih, iw = cvImage.shape[:-1]\n",
    "            imageHW = (ih, iw)\n",
    "\n",
    "            if (ih, iw) != (self.ieNetShape[2], self.ieNetShape[3]):\n",
    "                cvImage = cv2.resize(\n",
    "                    cvImage, (self.ieNetShape[3], self.ieNetShape[2]))\n",
    "\n",
    "            # Change data layout from HWC to CHW\n",
    "            cvImage = cvImage.transpose((2, 0, 1))\n",
    "            cvImage = cvImage.reshape(self.ieNetShape)\n",
    "\n",
    "            return cvImage\n",
    "        except:\n",
    "            PrintGetExceptionDetails()\n",
    "\n",
    "    def postprocess(self, infRes):\n",
    "        try:\n",
    "            detectedObjects = []\n",
    "\n",
    "            for obj in infRes[self.outBlob][0][0]:\n",
    "               dobj = {\n",
    "                   \"type\": \"entity\",\n",
    "                   \"entity\": {\n",
    "                       \"tag\": {\n",
    "                           \"value\": str(obj[1]),\n",
    "                           \"confidence\": str(obj[2])\n",
    "                       },\n",
    "                       \"box\": {\n",
    "                           \"l\": str(obj[3]),\n",
    "                           \"t\": str(obj[4]),\n",
    "                           \"w\": str(obj[5]-obj[3]),\n",
    "                           \"h\": str(obj[6]-obj[4])\n",
    "                       }\n",
    "                   }\n",
    "               }\n",
    "               detectedObjects.append(dobj)\n",
    "\n",
    "            return detectedObjects\n",
    "\n",
    "        except:\n",
    "            PrintGetExceptionDetails()\n",
    "\n",
    "    def score(self, cvImage):\n",
    "        try:\n",
    "            with self._lock:\n",
    "                image = self.preprocess(cvImage)\n",
    "                infRes = self.ieExecNet.infer(inputs={self.inputBlob: image})\n",
    "\n",
    "            return self.postprocess(infRes)\n",
    "\n",
    "        except:\n",
    "            PrintGetExceptionDetails()\n",
    "\n",
    "    def about(self):\n",
    "        aboutString = \"ModelName: {0}<br>ModelPrecision: {1}<br>TargetDev: {2}<br>PluginPath: {3}<br>CpuExtensions: {4}\".format(\n",
    "            self.modelName, self.modelPrecision, self.targetDev, self.pluginPath, self.cpuExtensions)\n",
    "        return aboutString\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score` method of the inference engine class above will return a dictionary of inferences in the following form:\n",
    "\n",
    "```\n",
    "      {\n",
    "        \"type\": \"entity\",\n",
    "        \"entity\": {\n",
    "          \"tag\": {\n",
    "            \"value\": \"1.0\",\n",
    "            \"confidence\": \"0.9596136\"\n",
    "          },\n",
    "          \"box\": {\n",
    "            \"l\": \"0.69242793\",\n",
    "            \"t\": \"0.3647236\",\n",
    "            \"w\": \"0.08401036\",\n",
    "            \"h\": \"0.07765585\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"entity\",\n",
    "        \"entity\": {\n",
    "          \"tag\": {\n",
    "            \"value\": \"1.0\",\n",
    "            \"confidence\": \"0.92975134\"\n",
    "          },\n",
    "          \"box\": {\n",
    "            \"l\": \"0.5211431\",\n",
    "            \"t\": \"0.44633362\",\n",
    "            \"w\": \"0.16630614\",\n",
    "            \"h\": \"0.12689856\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If all the code cells above have successfully finished running, return to the Readme page to continue.   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
