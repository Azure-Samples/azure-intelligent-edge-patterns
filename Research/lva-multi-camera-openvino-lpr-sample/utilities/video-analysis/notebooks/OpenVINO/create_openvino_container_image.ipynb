{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Local Docker Image\n",
    "In this section, we will create an IoT Edge module, a Docker container image with an HTTP web server that has a scoring REST endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from env_variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Web Application & Inference Server for Our ML Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To change the inference model that will be used in this sample, change the variable `IS_MODEL_NAME` based on your preferred model. Note that the model must be downloaded, as instructed in the [previous section](create_openvino_inference_engine.ipynb). The default model is         \"person-vehicle-bike-detection-crossroad-1016\".\n",
    "\n",
    "2. The variable assignment of `IS_TARGET_DEVICE` indicates what type of hardware acceleration you would like to use on your IoT Edge device. You can choose from the following choices:\n",
    "    * \"CPU\" for Intel® CPU acceleration  \n",
    "    * \"MYRIAD\" for Intel® VPU acceleration\n",
    "    * \"GPU\" for Intel® GPU acceleration\n",
    "    * \"FPGA\" for Intel® FPGA acceleration\n",
    "\n",
    "    Change the variable `IS_TARGET_DEVICE` as needed; the default is \"CPU\". \n",
    "\n",
    "3. The variable assignment of `IS_MODEL_PRECISION` indicates what type of model precision you would like to use. The default is \"FP32\" for this sample. However, please [check the Intel documentation](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html) for which output precision is supported for your desired hardware acceleration.\n",
    "\n",
    "4. Insert the subscription key for your Azure Cognitive Services Optical Character Recognition (OCR) instance (see https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-recognizing-text) on the `VISION-API-SUBSCRIPTION-KEY` placeholder on the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/app.py\n",
    "import threading\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import linecache\n",
    "from score import MLModel, PrintGetExceptionDetails\n",
    "from flask import Flask, request, Response, send_file, make_response\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import http.client\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import requests\n",
    "import io\n",
    "import json\n",
    "import base64\n",
    "\n",
    "\n",
    "def license_plate_detection_on_frame(cvImage):\n",
    "    output_path = \"/home/visionadmin/source/cropped_license_plates\"\n",
    "    detectedObjects = inferenceEngine.score(cvImage)\n",
    "    if len(detectedObjects) > 0:\n",
    "        confidence = 0.9\n",
    "        results = []\n",
    "        height, width, channels = cvImage.shape\n",
    "        for detected in detectedObjects:\n",
    "            label = float(detected[\"entity\"][\"tag\"][\"value\"])\n",
    "            if label == 2.0: # 1.0=Car; 2.0=Licence plate\n",
    "                conf = float(detected[\"entity\"][\"tag\"][\"confidence\"])\n",
    "                if conf > confidence:\n",
    "                    results.append(detected)\n",
    "        for result in results:\n",
    "            bbox = result['entity']['box']\n",
    "            left = float(bbox['l'])\n",
    "            top = float(bbox['t'])\n",
    "            bbox_width = float(bbox['w'])\n",
    "            bbox_height = float(bbox['h'])\n",
    "            left = math.ceil(left * width)\n",
    "            top = math.ceil(top * height)\n",
    "            bbox_width = math.ceil(float(bbox['w']) * width)\n",
    "            bbox_height = math.ceil(float(bbox['h']) * height)\n",
    "            bottom = top + bbox_height\n",
    "            right = left + bbox_width\n",
    "            color = (255, 0, 0)  # Blue in BGR\n",
    "            top_left = (left, top)\n",
    "            bottom_right = (right, bottom)\n",
    "            thickness = 2\n",
    "            cvImage = cv2.rectangle(cvImage, top_left, bottom_right, color, thickness)\n",
    "            crop_image = cvImage[top:bottom, left:right]\n",
    "            if crop_image.size > 0:\n",
    "                file_name = \"crop_\" + str(top) + \"_\" + str(left) + \".jpg\"\n",
    "                output = os.path.join(output_path, file_name)\n",
    "                cv2.imwrite(output, crop_image)\n",
    "    return cvImage\n",
    "\n",
    "\n",
    "def openvino_license_plate_recognition_on_frame(cvImage):\n",
    "    output_path = \"/home/visionadmin/source/cropped_license_plates\"\n",
    "    detectedObjects = inferenceEngine.score(cvImage)\n",
    "    if len(detectedObjects) > 0:\n",
    "        confidence = 0.9\n",
    "        results = []\n",
    "        height, width, channels = cvImage.shape\n",
    "        for detected in detectedObjects:\n",
    "            label = float(detected[\"entity\"][\"tag\"][\"value\"])\n",
    "            if label == 2.0: # 1.0=Car; 2.0=Licence plate\n",
    "                conf = float(detected[\"entity\"][\"tag\"][\"confidence\"])\n",
    "                if conf > confidence:\n",
    "                    results.append(detected)\n",
    "        recogEngine = MLModel(modelName=RECOGNITION_MODEL_NAME,\n",
    "                            modelPrecision=IS_MODEL_PRECISION, \n",
    "                            targetDev=IS_TARGET_DEVICE)\n",
    "        for result in results:\n",
    "            bbox = result['entity']['box']\n",
    "            left = float(bbox['l'])\n",
    "            top = float(bbox['t'])\n",
    "            bbox_width = float(bbox['w'])\n",
    "            bbox_height = float(bbox['h'])\n",
    "            left = math.ceil(left * width)\n",
    "            top = math.ceil(top * height)\n",
    "            bbox_width = math.ceil(float(bbox['w']) * width)\n",
    "            bbox_height = math.ceil(float(bbox['h']) * height)\n",
    "            bottom = top + bbox_height\n",
    "            right = left + bbox_width\n",
    "            color = (255, 0, 0)  # Blue in BGR\n",
    "            top_left = (left, top)\n",
    "            bottom_right = (right, bottom)\n",
    "            thickness = 2\n",
    "            cvImage = cv2.rectangle(cvImage, top_left, bottom_right, color, thickness)\n",
    "            crop_image = cvImage[top:bottom, left:right]\n",
    "            if crop_image.size > 0:\n",
    "                file_name = \"crop_\" + str(top) + \"_\" + str(left) + \".jpg\"\n",
    "                output = os.path.join(output_path, file_name)\n",
    "                cv2.imwrite(output, crop_image)\n",
    "                h, w, c = crop_image.shape\n",
    "                crop_image = np.reshape(crop_image, (c, h, w))\n",
    "                plates = recogEngine.score(crop_image)\n",
    "                scored = []\n",
    "                for plate in plates:\n",
    "                    conf = float(detected[\"entity\"][\"tag\"][\"confidence\"])\n",
    "                    if conf > 0.2:\n",
    "                        scored.append(plate)\n",
    "    detectedObjects = inferenceEngine.score(cvImage)\n",
    "    return cvImage\n",
    "\n",
    "\n",
    "def openvino_cognitiveservices_license_plate_recognition_on_frame(cvImage):\n",
    "    output_path = \"/home/visionadmin/source/cropped_license_plates\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/octet-stream', #  multipart/form-data\n",
    "        'Ocp-Apim-Subscription-Key': '<SUBSCRIPTION-KEY>'\n",
    "    }\n",
    "    detectedObjects = inferenceEngine.score(cvImage)\n",
    "    if len(detectedObjects) > 0:\n",
    "        confidence = 0.3\n",
    "        results = []\n",
    "        height, width, channels = cvImage.shape\n",
    "        for detected in detectedObjects:\n",
    "            label = float(detected[\"entity\"][\"tag\"][\"value\"])\n",
    "            if label == 2.0: # 1.0=Car; 2.0=Licence plate\n",
    "                conf = float(detected[\"entity\"][\"tag\"][\"confidence\"])\n",
    "                if conf > confidence:\n",
    "                    results.append(detected)\n",
    "        for result in results:\n",
    "            bbox = result['entity']['box']\n",
    "            left = float(bbox['l'])\n",
    "            top = float(bbox['t'])\n",
    "            bbox_width = float(bbox['w'])\n",
    "            bbox_height = float(bbox['h'])\n",
    "            left = math.ceil(left * width)\n",
    "            top = math.ceil(top * height)\n",
    "            bbox_width = math.ceil(float(bbox['w']) * width)\n",
    "            bbox_height = math.ceil(float(bbox['h']) * height)\n",
    "            bottom = top + bbox_height\n",
    "            right = left + bbox_width\n",
    "            color = (255, 0, 0)  # Blue in BGR\n",
    "            top_left = (left, top)\n",
    "            bottom_right = (right, bottom)\n",
    "            thickness = 2\n",
    "            cvImage = cv2.rectangle(cvImage, top_left, bottom_right, color, thickness)\n",
    "            crop_image = cvImage[top:bottom, left:right]\n",
    "            if crop_image.size > 0:\n",
    "                file_name = \"crop_\" + str(top) + \"_\" + str(left) + \".jpg\"\n",
    "                output = os.path.join(output_path, file_name)\n",
    "                h, w, c = crop_image.shape\n",
    "                if h < 50:\n",
    "                    ratio = w / h\n",
    "                    new_h = 60\n",
    "                    new_w = math.ceil(new_h * ratio)\n",
    "                    crop_image = cv2.resize(crop_image, (new_w, new_h), cv2.INTER_AREA)\n",
    "                cv2.imwrite(output, crop_image)\n",
    "                try:\n",
    "                    encoded_image = cv2.imencode(\".jpg\", crop_image)[1].tobytes()\n",
    "                    params = {\n",
    "                        'language': 'unk',\n",
    "                        'detectOrientation': 'true'\n",
    "                    }\n",
    "                    ocr_url = \"https://aivisiontests.cognitiveservices.azure.com/vision/v3.0/ocr\"\n",
    "                    response = requests.post(ocr_url,\n",
    "                    headers=headers,\n",
    "                    params=params,\n",
    "                    data=encoded_image\n",
    "                    )\n",
    "                    content = json.loads(response.content)\n",
    "                    regions = content[\"regions\"]\n",
    "                    if regions is not None:\n",
    "                        for region in regions:\n",
    "                            lines = region[\"lines\"]\n",
    "                            text = \"\"\n",
    "                            for line in lines:\n",
    "                                words = line[\"words\"]\n",
    "                                text = \"\"\n",
    "                                for word in words:\n",
    "                                    bbox = [int(num) for num in word[\"boundingBox\"].split(\",\")]\n",
    "                                    text = text + word[\"text\"].upper()\n",
    "                                    print(\"Bounding box: {0} {1}\".format(bbox[2], bbox[3]))\n",
    "                                    print(\"Text: {}\".format(text))\n",
    "                                    origin = (bottom, left - 10)\n",
    "                            if text is not None:\n",
    "                                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                                fontScale = 2\n",
    "                                thickness = 2\n",
    "                                cvImage = cv2.putText(\n",
    "                                    cvImage,\n",
    "                                    text,\n",
    "                                    origin,\n",
    "                                    font,\n",
    "                                    fontScale,\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness,\n",
    "                                    cv2.LINE_AA\n",
    "                                )\n",
    "                except Exception as ex:\n",
    "                    print(\"Error:\", str(ex))                \n",
    "    return cvImage\n",
    "\n",
    "\n",
    "def car_detection_on_frame(cvImage):\n",
    "    output_path = \"/home/visionadmin/source/cropped_license_plates\"\n",
    "    detectedObjects = inferenceEngine.score(cvImage)\n",
    "    if len(detectedObjects) > 0:\n",
    "        confidence = 0.3\n",
    "        results = []\n",
    "        height, width, channels = cvImage.shape\n",
    "        for detected in detectedObjects:\n",
    "            label = float(detected[\"entity\"][\"tag\"][\"value\"])\n",
    "            if label == 1.0: # 1.0=Car; 2.0=Licence plate\n",
    "                conf = float(detected[\"entity\"][\"tag\"][\"confidence\"])\n",
    "                if conf > confidence:\n",
    "                    results.append(detected)\n",
    "        for result in results:\n",
    "            bbox = result['entity']['box']\n",
    "            left = float(bbox['l'])\n",
    "            top = float(bbox['t'])\n",
    "            bbox_width = float(bbox['w'])\n",
    "            bbox_height = float(bbox['h'])\n",
    "            left = math.ceil(left * width)\n",
    "            top = math.ceil(top * height)\n",
    "            bbox_width = math.ceil(float(bbox['w']) * width)\n",
    "            bbox_height = math.ceil(float(bbox['h']) * height)\n",
    "            bottom = top + bbox_height\n",
    "            right = left + bbox_width\n",
    "            color = (255, 0, 0)  # Blue in BGR\n",
    "            top_left = (left, top)\n",
    "            bottom_right = (right, bottom)\n",
    "            thickness = 2\n",
    "            cvImage = cv2.rectangle(cvImage, top_left, bottom_right, color, thickness)\n",
    "    return cvImage\n",
    "\n",
    "\n",
    "def lpr_recognition_from_frame(frame):\n",
    "    frame = car_detection_on_frame(frame)\n",
    "    frame = openvino_cognitiveservices_license_plate_recognition_on_frame(frame)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def uncanny_detection_from_frame(cvImage):\n",
    "    response = None\n",
    "    file_name = \"frame.jpg\"\n",
    "    cv2.imwrite(file_name, cvImage)\n",
    "    try:\n",
    "        url = \"http://52.188.213.201:5001/api/upload_event\"\n",
    "        files = {\n",
    "            'file': open(file_name, 'rb')\n",
    "        }\n",
    "        response = requests.post(url, files=files)\n",
    "        print(response.text)\n",
    "        detectedObjects = json.loads(response.text)\n",
    "        detectedObjects = detectedObjects['inferences']\n",
    "        return detectedObjects\n",
    "    except Exception as ex:\n",
    "        print(\"UnCanny error:\", str(ex))\n",
    "    return []\n",
    "\n",
    "\n",
    "# Initial settings of AI model\n",
    "#IS_MODEL_NAME = \"person-vehicle-bike-detection-crossroad-1016\" # see MLModel class for full list of models and other possibilities\n",
    "IS_MODEL_NAME = \"vehicle-license-plate-detection-barrier-0106\" # see MLModel class for full list of models and other possibilities\n",
    "RECOGNITION_MODEL_NAME = \"license-plate-recognition-barrier-0001\"\n",
    "IS_TARGET_DEVICE = \"CPU\"\n",
    "IS_MODEL_PRECISION = \"FP32\"\n",
    "MODEL_PRECISION_16 = \"FP16\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "inferenceEngine = MLModel(    modelName=IS_MODEL_NAME, \n",
    "                                modelPrecision=IS_MODEL_PRECISION, \n",
    "                                targetDev=IS_TARGET_DEVICE, \n",
    "                            )\n",
    "\n",
    "\n",
    "@app.route('/stream/<id>')\n",
    "def stream(id):\n",
    "    respBody = (\"<html>\"\n",
    "               \"<h1>Stream with inferencing overlays</h1>\"\n",
    "               \"img src=\\\"/mjpeg/\" + id + \"\\\"/>\"\n",
    "               \"</html>\")\n",
    "    return Response(respBody, status=200)\n",
    "\n",
    "\n",
    "@app.route(\"/score\", methods = ['POST'])\n",
    "def scoreRRS():\n",
    "    global inferenceEngine\n",
    " \n",
    "    try:\n",
    "        # get request as byte stream\n",
    "        reqBody = request.get_data(False)\n",
    "        # convert from byte stream\n",
    "        inMemFile = io.BytesIO(reqBody)\n",
    "        # load a sample image\n",
    "        inMemFile.seek(0)\n",
    "        fileBytes = np.asarray(bytearray(inMemFile.read()), dtype=np.uint8)\n",
    "        cvImage = cv2.imdecode(fileBytes, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if request.args:\n",
    "            stream = request.args.get('stream')\n",
    "        try:\n",
    "            if stream is not None:\n",
    "                output_img = lpr_recognition_from_frame(cvImage)\n",
    "                output_img = Image.fromarray(output_img)\n",
    "                imgBuf = io.BytesIO()\n",
    "                output_img.save(imgBuf, format='JPEG')\n",
    "                headers = {'Content-Type': 'image/jpeg'}\n",
    "                # post the image with bounding boxes so that it can be viewed as an MJPEG stream\n",
    "                postData = b'--boundary\\r\\n' + b'Content-Type: image/jpeg\\r\\n\\r\\n' + imgBuf.getvalue() + b'\\r\\n'\n",
    "                requests.post('http://127.0.0.1:5001/mjpeg_pub/' + stream, data = postData)\n",
    "        except Exception as ex:\n",
    "            print('EXCEPTION:', str(ex))\n",
    "                \n",
    "        # call scoring function\n",
    "        detectedObjects = inferenceEngine.score(cvImage)\n",
    "        if len(detectedObjects) > 0:\n",
    "            confidence = 0.3\n",
    "            results = []\n",
    "            for detected in detectedObjects:\n",
    "                conf = float(detected[\"entity\"][\"tag\"][\"confidence\"])\n",
    "                if  conf > confidence:\n",
    "                    results.append(detected)\n",
    "            respBody = {                    \n",
    "                        \"inferences\" : results\n",
    "                    }\n",
    "\n",
    "            logging.info(\"[AI EXT] Sending response.\")\n",
    "            respBody = json.dumps(respBody)\n",
    "            return Response(respBody, status= 200, mimetype ='application/json')\n",
    "        else:\n",
    "            logging.info(\"[AI EXT] Sending empty response.\")\n",
    "            return Response(status= 204)\n",
    "\n",
    "    except:\n",
    "        PrintGetExceptionDetails()\n",
    "        return Response(response='Exception occured while processing the image.', status=500)\n",
    "    \n",
    "@app.route(\"/\")\n",
    "def healthy():\n",
    "    return \"Healthy\"\n",
    "\n",
    "# About\n",
    "@app.route('/about', methods = ['GET'])\n",
    "def about_request():\n",
    "    global inferenceEngine\n",
    "    return inferenceEngine.about()\n",
    "\n",
    "if __name__ == \"__main__\":      \n",
    "    app.run(host='127.0.0.1', port=5444)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, 5444 is the internal port of the webserver app that listens the requests. Next, we will map it to different ports to expose it externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/wsgi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/wsgi.py\n",
    "from app import app as application\n",
    "\n",
    "def create():\n",
    "    application.run(host='127.0.0.1', port=5444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(isSolutionPath, \"nginx\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exposed port of the web app is now 5001, while the internal one is still 5444."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/nginx/app\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/nginx/app\n",
    "server {\n",
    "    listen 5001;\n",
    "    server_name _;\n",
    " \n",
    "    location / {\n",
    "        include proxy_params;\n",
    "        proxy_pass http://127.0.0.1:5444;\n",
    "        proxy_connect_timeout 5000s;\n",
    "        proxy_read_timeout 5000s;\n",
    "    }\n",
    "    \n",
    "     location ~ /mjpeg_pub/(\\w+)$ {\n",
    "         nchan_publisher;\n",
    "         nchan_channel_id $1;\n",
    "         nchan_channel_group mjpeg;\n",
    "         nchan_message_buffer_length 5;\n",
    "         nchan_message_timeout 10s;\n",
    "     }\n",
    "\n",
    "    location ~ /mjpeg/(\\w+)$ {\n",
    "        add_header Content-Type \"multipart/x-mixed-replace; boundary=--boundary\";\n",
    "        nchan_subscriber http-raw-stream;\n",
    "        nchan_channel_id $1;\n",
    "        nchan_channel_group mjpeg;\n",
    "        nchan_subscriber_first_message newest;\n",
    "    }        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/gunicorn_logging.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/gunicorn_logging.conf\n",
    "\n",
    "[loggers]\n",
    "keys=root, gunicorn.error\n",
    "\n",
    "[handlers]\n",
    "keys=console\n",
    "\n",
    "[formatters]\n",
    "keys=json\n",
    "\n",
    "[logger_root]\n",
    "level=INFO\n",
    "handlers=console\n",
    "\n",
    "[logger_gunicorn.error]\n",
    "level=ERROR\n",
    "handlers=console\n",
    "propagate=0\n",
    "qualname=gunicorn.error\n",
    "\n",
    "[handler_console]\n",
    "class=StreamHandler\n",
    "formatter=json\n",
    "args=(sys.stdout, )\n",
    "\n",
    "[formatter_json]\n",
    "class=jsonlogging.JSONFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/kill_supervisor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/kill_supervisor.py\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "\n",
    "def write_stdout(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# this function is modified from the code and knowledge found here: http://supervisord.org/events.html#example-event-listener-implementation\n",
    "def main():\n",
    "    while 1:\n",
    "        write_stdout('[AI EXT] READY\\n')\n",
    "        # wait for the event on stdin that supervisord will send\n",
    "        line = sys.stdin.readline()\n",
    "        write_stdout('[AI EXT] Terminating supervisor with this event: ' + line);\n",
    "        try:\n",
    "            # supervisord writes its pid to its file from which we read it here, see supervisord.conf\n",
    "            pidfile = open('/tmp/supervisord.pid','r')\n",
    "            pid = int(pidfile.readline());\n",
    "            os.kill(pid, signal.SIGQUIT)\n",
    "        except Exception as e:\n",
    "            write_stdout('[AI EXT] Could not terminate supervisor: ' + e.strerror + '\\n')\n",
    "            write_stdout('[AI EXT] RESULT 2\\nOK')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(isSolutionPath, \"etc\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/etc/supervisord.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/etc/supervisord.conf \n",
    "[supervisord]\n",
    "logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)\n",
    "logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\n",
    "logfile_backups=10           ; (num of main logfile rotation backups;default 10)\n",
    "loglevel=info                ; (log level;default info; others: debug,warn,trace)\n",
    "pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n",
    "nodaemon=true                ; (start in foreground if true;default false)\n",
    "minfds=1024                  ; (min. avail startup file descriptors;default 1024)\n",
    "minprocs=200                 ; (min. avail process descriptors;default 200)\n",
    "\n",
    "environment=LD_LIBRARY_PATH=%(ENV_LD_LIBRARY_PATH)s,INTEL_CVSDK_DIR=%(ENV_INTEL_CVSDK_DIR)s,OpenCV_DIR=%(ENV_OpenCV_DIR)s,InferenceEngine_DIR=%(ENV_InferenceEngine_DIR)s,PYTHONPATH=%(ENV_PYTHONPATH)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s,HDDL_INSTALL_DIR=%(ENV_HDDL_INSTALL_DIR)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s\n",
    "\n",
    "[program:gunicorn]\n",
    "command=bash -c \"gunicorn --workers 1 -m 007 --timeout 100000 --capture-output --error-logfile - --log-level debug --log-config gunicorn_logging.conf \\\"wsgi:create()\\\"\"\n",
    "directory=/isserver\n",
    "redirect_stderr=true\n",
    "stdout_logfile =/dev/stdout\n",
    "stdout_logfile_maxbytes=0\n",
    "startretries=2\n",
    "startsecs=20\n",
    "\n",
    "[program:nginx]\n",
    "command=/usr/sbin/nginx -g \"daemon off;\"\n",
    "startretries=2\n",
    "startsecs=5\n",
    "priority=3\n",
    "\n",
    "[eventlistener:program_exit]\n",
    "command=python kill_supervisor.py\n",
    "directory=/isserver\n",
    "events=PROCESS_STATE_FATAL\n",
    "priority=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/requirements.txt\n",
    "pillow<7.0.0\n",
    "click==6.7\n",
    "configparser==3.5.0\n",
    "Flask==0.12.2\n",
    "gunicorn==19.6.0\n",
    "json-logging-py==0.2\n",
    "MarkupSafe==1.0\n",
    "olefile==0.44\n",
    "requests==2.12.3\n",
    "urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker File to Containerize the ML Solution and Web App Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!IMPORTANT] </span>  \n",
    "\n",
    "> The OpenVINO™ Toolkit is a licenced software. To ensure that you are using the latest version of the OpenVINO™ Toolkit, follow these instructions to obtain a licensed download link:  \n",
    "\n",
    "> 1) Go to the [Intel donwload link](https://software.intel.com/en-us/openvino-toolkit/choose-download/free-download-linux) for the OpenVINO™ Toolkit\n",
    "\n",
    "> 2) Click on the \"Register & Download\" button  \n",
    "\n",
    "> <img src=\"../../../../images/_openvino_img_03_001.jpg\" width=400 alt=\"> Figure: Register & Download.\"/>  \n",
    "\n",
    "> 3) Fill in the form and click submit \n",
    "\n",
    "> <img src=\"../../../../images/_openvino_img_03_002.jpg\" width=400 alt=\"> Figure: Fill the form.\"/>  \n",
    "\n",
    "> 4) Over the \"Full Package\" link, right click and get the link which should look like something:  \n",
    "    http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE\\>/l_openvino_toolkit_p_2020.3.194.tgz  \n",
    "    \n",
    "> <img src=\"../../../../images/_openvino_img_03_003.jpg\" width=400 alt=\"> Figure: Download link.\"/>  \n",
    "\n",
    "> 5) In the below cell, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As described above, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have (below is sample URI, just remove it and use your own address)\n",
    "\n",
    "# openVinoToolkitDownloadLink = \"http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE>/l_openvino_toolkit_p_2020.3.194.tgz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/Dockerfile\n",
    "\n",
    "FROM ubuntu:18.04\n",
    "\n",
    "USER root\n",
    "\n",
    "ARG WORK_DIR=/isserver\n",
    "ENV WORK_DIR ${WORK_DIR}\n",
    "ENV PATH /opt/miniconda/bin:${PATH}\n",
    "\n",
    "RUN mkdir -p ${WORK_DIR}\n",
    "\n",
    "WORKDIR ${WORK_DIR}\n",
    "\n",
    "#\n",
    "# Install base\n",
    "#\n",
    "RUN apt-get update &&\\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "        # Essentials\n",
    "        wget \\\n",
    "        locales \\\n",
    "        # Python environment\n",
    "        python3 \\\n",
    "        python3-setuptools &&\\\n",
    "    #\n",
    "    # Dependencies: conda\n",
    "    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate &&\\ \n",
    "    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda &&\\\n",
    "    #\n",
    "    # Cleaning\n",
    "    /opt/miniconda/bin/conda clean -ya &&\\\n",
    "    rm -rf /opt/miniconda/pkgs &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "#\n",
    "# Install OpenVINO™\n",
    "#\n",
    "#COPY l_openvino_toolkit_p_2020.1.023.tgz ${WORK_DIR}\n",
    "RUN apt-get update &&\\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "        # Essentials\n",
    "        cpio \\\n",
    "        udev \\\n",
    "        unzip \\\n",
    "        autoconf \\\n",
    "        automake \\\n",
    "        libtool\n",
    "\n",
    "RUN wget --quiet IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK -O ${WORK_DIR}/l_openvino_toolkit_p_2020.1.023.tgz &&\\\n",
    "    pattern=\"COMPONENTS=DEFAULTS\" &&\\\n",
    "    replacement=\"COMPONENTS=intel-openvino-ie-sdk-ubuntu-bionic__x86_64;intel-openvino-ie-rt-cpu-ubuntu-bionic__x86_64;intel-openvino-ie-rt-vpu-ubuntu-bionic__x86_64;intel-openvino-opencv-lib-ubuntu-bionic__x86_64\" &&\\\n",
    "    tar -xzf l_openvino_toolkit*.tgz &&\\\n",
    "    rm -rf l_openvino_toolkit*.tgz &&\\\n",
    "    cd l_openvino_toolkit* &&\\\n",
    "    sed -i \"s/$pattern/$replacement/\" silent.cfg &&\\\n",
    "    sed -i \"s/decline/accept/g\" silent.cfg &&\\\n",
    "    /bin/bash ./install.sh -s silent.cfg &&\\\n",
    "    cd - &&\\\n",
    "    cd /opt/intel/openvino/install_dependencies &&\\\n",
    "    /bin/bash ./install_openvino_dependencies.sh &&\\\n",
    "    # setup environment variables\n",
    "    echo \"source /opt/intel/openvino/bin/setupvars.sh\" >> /root/.bashrc &&\\\n",
    "    #\n",
    "    # Cleaning\n",
    "    cd ${WORK_DIR} &&\\\n",
    "    rm -rf * &&\\\n",
    "    /opt/miniconda/bin/conda clean -ya &&\\\n",
    "    rm -rf /opt/miniconda/pkgs &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "#\n",
    "# Set environment variables as in ${INTEL_OPENVINO_DIR}/bin/setupvars.sh\n",
    "ENV INTEL_OPENVINO_DIR /opt/intel/openvino\n",
    "ENV LD_LIBRARY_PATH ${INTEL_OPENVINO_DIR}/opencv/lib:${INTEL_OPENVINO_DIR}/deployment_tools/ngraph/lib:/opt/intel/opencl:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/gna/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/tbb/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64\n",
    "ENV INTEL_CVSDK_DIR ${INTEL_OPENVINO_DIR}\n",
    "ENV OpenCV_DIR ${INTEL_OPENVINO_DIR}/opencv/cmake\n",
    "ENV InferenceEngine_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/share\n",
    "ENV PYTHONPATH ${INTEL_OPENVINO_DIR}/python/python3.7:${INTEL_OPENVINO_DIR}/python/python3:${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker:${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer\n",
    "ENV PATH ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:${PATH}\n",
    "ENV HDDL_INSTALL_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl\n",
    "\n",
    "#\n",
    "# Exclude UDEV by rebuilding libusb without UDEV support\n",
    "RUN cp ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/97-myriad-usbboot.rules /etc/udev/rules.d/ &&\\\n",
    "    ldconfig &&\\\n",
    "    cd /opt && wget --quiet --no-check-certificate http://github.com/libusb/libusb/archive/v1.0.22.zip -O /opt/v1.0.22.zip &&\\\n",
    "    unzip v1.0.22.zip && cd libusb-1.0.22 &&\\\n",
    "    ./bootstrap.sh &&\\\n",
    "    ./configure --disable-udev --enable-shared &&\\\n",
    "    make -j4\n",
    "\n",
    "RUN apt-get update &&\\\n",
    "    apt-get install -y --no-install-recommends libusb-1.0-0-dev &&\\\n",
    "    cd /opt &&\\\n",
    "    rm -rf /var/lib/apt/lists/* &&\\\n",
    "    cd /opt/libusb-1.0.22/libusb &&\\\n",
    "    /bin/mkdir -p '/usr/local/lib' &&\\\n",
    "    /bin/bash ../libtool --mode=install /usr/bin/install -c libusb-1.0.la '/usr/local/lib' &&\\\n",
    "    /bin/mkdir -p '/usr/local/include/libusb-1.0' &&\\\n",
    "    /usr/bin/install -c -m 644 libusb.h '/usr/local/include/libusb-1.0' &&\\\n",
    "    /bin/mkdir -p '/usr/local/lib/pkgconfig' &&\\\n",
    "    cd /opt/libusb-1.0.22/ &&\\\n",
    "    /usr/bin/install -c -m 644 libusb-1.0.pc '/usr/local/lib/pkgconfig' &&\\\n",
    "    ldconfig\n",
    "\n",
    "#\n",
    "# Install ML solution\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    nginx \\\n",
    "    supervisor &&\\\n",
    "    pip install \\\n",
    "        numpy \\\n",
    "        azure-iot-device\n",
    "\n",
    "# Install Nchan module\n",
    "RUN apt-get update -y &&\\\n",
    "    apt-get install -y libnginx-mod-nchan &&\\\n",
    "    /etc/init.d/nginx restart\n",
    "\n",
    "ADD . ${WORK_DIR}\n",
    "ADD etc /etc\n",
    "\n",
    "RUN rm -rf /var/lib/apt/lists/* &&\\\n",
    "    rm /etc/nginx/sites-enabled/default &&\\\n",
    "    cp ${WORK_DIR}/nginx/app /etc/nginx/sites-available/ &&\\\n",
    "    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ &&\\\n",
    "    pip install -r ${WORK_DIR}/requirements.txt &&\\\n",
    "    /opt/miniconda/bin/conda clean -ya &&\\\n",
    "    rm -rf /opt/miniconda/pkgs &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "EXPOSE 5001 8080\n",
    "CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Docker file with custom environment variable: IoT Edge device's connection string\n",
    "filePath = isSolutionPath+\"/Dockerfile\"\n",
    "file = open(filePath)\n",
    "dockerFileTemplate = file.read()\n",
    "dockerFileTemplate = dockerFileTemplate.replace(\"IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK\", \"\\\"\"+openVinoToolkitDownloadLink+\"\\\"\")\n",
    "\n",
    "with open(filePath, 'wt', encoding='utf-8') as outputFile:\n",
    "    outputFile.write(dockerFileTemplate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Local Docker Image\n",
    "Finally, we will create a Docker image locally. We will later host the image in a container registry like Docker Hub, Azure Container Registry, or a local registry.\n",
    "\n",
    "To run the following code snippet, you must have the pre-requisities mentioned in [the requirements page](../common/requirements.md). Most notably, we are running the `docker` command without `sudo`.\n",
    "\n",
    "> <span>[!WARNING]</span>\n",
    "> Please ensure that Docker is running before executing the cell below. Execution of the cell below may take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  202.1MB\n",
      "Step 1/27 : FROM ubuntu:18.04\n",
      " ---> d27b9ffc5667\n",
      "Step 2/27 : USER root\n",
      " ---> Using cache\n",
      " ---> 30fea6fec12d\n",
      "Step 3/27 : ARG WORK_DIR=/isserver\n",
      " ---> Using cache\n",
      " ---> e051f506fe89\n",
      "Step 4/27 : ENV WORK_DIR ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> 5800225e4748\n",
      "Step 5/27 : ENV PATH /opt/miniconda/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> 5d2b2bed46a5\n",
      "Step 6/27 : RUN mkdir -p ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> 1d82213511af\n",
      "Step 7/27 : WORKDIR ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> b83b980f1afa\n",
      "Step 8/27 : RUN apt-get update &&    apt-get install -y --no-install-recommends         wget         locales         python3         python3-setuptools &&    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate &&    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda &&    /opt/miniconda/bin/conda clean -ya &&    rm -rf /opt/miniconda/pkgs &&    rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> db424a27336c\n",
      "Step 9/27 : RUN apt-get update &&    apt-get install -y --no-install-recommends         cpio         udev         unzip         autoconf         automake         libtool\n",
      " ---> Using cache\n",
      " ---> b7f84eca0df8\n",
      "Step 10/27 : RUN wget --quiet \"http://registrationcenter-download.intel.com/akdlm/irc_nas/16670/l_openvino_toolkit_p_2020.3.194.tgz\" -O ${WORK_DIR}/l_openvino_toolkit_p_2020.1.023.tgz &&    pattern=\"COMPONENTS=DEFAULTS\" &&    replacement=\"COMPONENTS=intel-openvino-ie-sdk-ubuntu-bionic__x86_64;intel-openvino-ie-rt-cpu-ubuntu-bionic__x86_64;intel-openvino-ie-rt-vpu-ubuntu-bionic__x86_64;intel-openvino-opencv-lib-ubuntu-bionic__x86_64\" &&    tar -xzf l_openvino_toolkit*.tgz &&    rm -rf l_openvino_toolkit*.tgz &&    cd l_openvino_toolkit* &&    sed -i \"s/$pattern/$replacement/\" silent.cfg &&    sed -i \"s/decline/accept/g\" silent.cfg &&    /bin/bash ./install.sh -s silent.cfg &&    cd - &&    cd /opt/intel/openvino/install_dependencies &&    /bin/bash ./install_openvino_dependencies.sh &&    echo \"source /opt/intel/openvino/bin/setupvars.sh\" >> /root/.bashrc &&    cd ${WORK_DIR} &&    rm -rf * &&    /opt/miniconda/bin/conda clean -ya &&    rm -rf /opt/miniconda/pkgs &&    rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> d975c99acd42\n",
      "Step 11/27 : ENV INTEL_OPENVINO_DIR /opt/intel/openvino\n",
      " ---> Using cache\n",
      " ---> 03a6921eefaa\n",
      "Step 12/27 : ENV LD_LIBRARY_PATH ${INTEL_OPENVINO_DIR}/opencv/lib:${INTEL_OPENVINO_DIR}/deployment_tools/ngraph/lib:/opt/intel/opencl:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/gna/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/tbb/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64\n",
      " ---> Using cache\n",
      " ---> ca62e51b81b9\n",
      "Step 13/27 : ENV INTEL_CVSDK_DIR ${INTEL_OPENVINO_DIR}\n",
      " ---> Using cache\n",
      " ---> 702ccc0b663b\n",
      "Step 14/27 : ENV OpenCV_DIR ${INTEL_OPENVINO_DIR}/opencv/cmake\n",
      " ---> Using cache\n",
      " ---> 3165ed2f077f\n",
      "Step 15/27 : ENV InferenceEngine_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/share\n",
      " ---> Using cache\n",
      " ---> dcb08ae895fb\n",
      "Step 16/27 : ENV PYTHONPATH ${INTEL_OPENVINO_DIR}/python/python3.7:${INTEL_OPENVINO_DIR}/python/python3:${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker:${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer\n",
      " ---> Using cache\n",
      " ---> 6c12737a6d0f\n",
      "Step 17/27 : ENV PATH ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> 9de6dd53de50\n",
      "Step 18/27 : ENV HDDL_INSTALL_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl\n",
      " ---> Using cache\n",
      " ---> 3c69fd36d1ec\n",
      "Step 19/27 : RUN cp ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/97-myriad-usbboot.rules /etc/udev/rules.d/ &&    ldconfig &&    cd /opt && wget --quiet --no-check-certificate http://github.com/libusb/libusb/archive/v1.0.22.zip -O /opt/v1.0.22.zip &&    unzip v1.0.22.zip && cd libusb-1.0.22 &&    ./bootstrap.sh &&    ./configure --disable-udev --enable-shared &&    make -j4\n",
      " ---> Using cache\n",
      " ---> 22dc66492665\n",
      "Step 20/27 : RUN apt-get update &&    apt-get install -y --no-install-recommends libusb-1.0-0-dev &&    cd /opt &&    rm -rf /var/lib/apt/lists/* &&    cd /opt/libusb-1.0.22/libusb &&    /bin/mkdir -p '/usr/local/lib' &&    /bin/bash ../libtool --mode=install /usr/bin/install -c libusb-1.0.la '/usr/local/lib' &&    /bin/mkdir -p '/usr/local/include/libusb-1.0' &&    /usr/bin/install -c -m 644 libusb.h '/usr/local/include/libusb-1.0' &&    /bin/mkdir -p '/usr/local/lib/pkgconfig' &&    cd /opt/libusb-1.0.22/ &&    /usr/bin/install -c -m 644 libusb-1.0.pc '/usr/local/lib/pkgconfig' &&    ldconfig\n",
      " ---> Using cache\n",
      " ---> c7fa9f6e4a52\n",
      "Step 21/27 : RUN apt-get update && apt-get install -y --no-install-recommends     nginx     supervisor &&    pip install         numpy         azure-iot-device\n",
      " ---> Using cache\n",
      " ---> e22257b5af4c\n",
      "Step 22/27 : RUN apt-get update -y &&    apt-get install -y libnginx-mod-nchan &&    /etc/init.d/nginx restart\n",
      " ---> Using cache\n",
      " ---> 3f7bbcadcb53\n",
      "Step 23/27 : ADD . ${WORK_DIR}\n",
      " ---> 36bed9b7a068\n",
      "Step 24/27 : ADD etc /etc\n",
      " ---> f3979aa26114\n",
      "Step 25/27 : RUN rm -rf /var/lib/apt/lists/* &&    rm /etc/nginx/sites-enabled/default &&    cp ${WORK_DIR}/nginx/app /etc/nginx/sites-available/ &&    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ &&    pip install -r ${WORK_DIR}/requirements.txt &&    /opt/miniconda/bin/conda clean -ya &&    rm -rf /opt/miniconda/pkgs &&    rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 4a6d8bddde4a\n",
      "Collecting pillow<7.0.0 (from -r /isserver/requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/3f/03375124676ab49ca6e6917c0f1f663afb8354d5d24e12f4fe4587a39ae2/Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
      "Collecting click==6.7 (from -r /isserver/requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n",
      "Collecting configparser==3.5.0 (from -r /isserver/requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/69/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c/configparser-3.5.0.tar.gz\n",
      "Collecting Flask==0.12.2 (from -r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB)\n",
      "Collecting gunicorn==19.6.0 (from -r /isserver/requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/72/de/ec28a64885e0b390063379cca601b60b1f9e51367e0c76030ac8a5cddd5e/gunicorn-19.6.0-py2.py3-none-any.whl (114kB)\n",
      "Collecting json-logging-py==0.2 (from -r /isserver/requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/e1/46c70eebf216b830867c4896ee678cb7f1b28bb68a2810c7e9a811cecfbc/json-logging-py-0.2.tar.gz\n",
      "Collecting MarkupSafe==1.0 (from -r /isserver/requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz\n",
      "Collecting olefile==0.44 (from -r /isserver/requirements.txt (line 8))\n",
      "  Downloading https://files.pythonhosted.org/packages/35/17/c15d41d5a8f8b98cc3df25eb00c5cee76193114c78e5674df6ef4ac92647/olefile-0.44.zip (74kB)\n",
      "Collecting requests==2.12.3 (from -r /isserver/requirements.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/84/68/f0acceafe80354aa9ff4ae49de0572d27929b6d262f0c55196424eb86b2f/requests-2.12.3-py2.py3-none-any.whl (575kB)\n",
      "Requirement already satisfied: urllib3 in /opt/miniconda/lib/python3.7/site-packages (from -r /isserver/requirements.txt (line 10)) (1.23)\n",
      "Collecting Jinja2>=2.4 (from Flask==0.12.2->-r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB)\n",
      "Collecting itsdangerous>=0.21 (from Flask==0.12.2->-r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Collecting Werkzeug>=0.7 (from Flask==0.12.2->-r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "Building wheels for collected packages: configparser, json-logging-py, MarkupSafe, olefile\n",
      "  Running setup.py bdist_wheel for configparser: started\n",
      "  Running setup.py bdist_wheel for configparser: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/61/79/424ef897a2f3b14684a7de5d89e8600b460b89663e6ce9d17c\n",
      "  Running setup.py bdist_wheel for json-logging-py: started\n",
      "  Running setup.py bdist_wheel for json-logging-py: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/2e/1c/c638b7589610d8b9358a6e5eb008edacb8b3e9b6d1edc9479f\n",
      "  Running setup.py bdist_wheel for MarkupSafe: started\n",
      "  Running setup.py bdist_wheel for MarkupSafe: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/56/20/ebe49a5c612fffe1c5a632146b16596f9e64676768661e4e46\n",
      "  Running setup.py bdist_wheel for olefile: started\n",
      "  Running setup.py bdist_wheel for olefile: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/19/76/61fc7929d808e51567aff23036ca5fe6ba8336ad0559ca6a27\n",
      "Successfully built configparser json-logging-py MarkupSafe olefile\n",
      "\u001b[91mazure-iot-device 2.1.4 has requirement requests<3.0.0,>=2.20.0, but you'll have requests 2.12.3 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: pillow, click, configparser, MarkupSafe, Jinja2, itsdangerous, Werkzeug, Flask, gunicorn, json-logging-py, olefile, requests\n",
      "  Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed Flask-0.12.2 Jinja2-2.11.2 MarkupSafe-1.0 Werkzeug-1.0.1 click-6.7 configparser-3.5.0 gunicorn-19.6.0 itsdangerous-1.1.0 json-logging-py-0.2 olefile-0.44 pillow-6.2.2 requests-2.12.3\n",
      "\u001b[91mYou are using pip version 10.0.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mCache location: \n",
      "There are no tarballs to remove\n",
      "Cache location: \n",
      "There are no unused packages to remove\n",
      "source cache (/opt/miniconda/conda-bld/src_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "git cache (/opt/miniconda/conda-bld/git_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "hg cache (/opt/miniconda/conda-bld/hg_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "svn cache (/opt/miniconda/conda-bld/svn_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "Total:                                          0 B\n",
      "Removing /opt/miniconda/conda-bld/src_cache\n",
      "Removing /opt/miniconda/conda-bld/git_cache\n",
      "Removing /opt/miniconda/conda-bld/hg_cache\n",
      "Removing /opt/miniconda/conda-bld/svn_cache\n",
      "Removing intermediate container 4a6d8bddde4a\n",
      " ---> 659afcb12d00\n",
      "Step 26/27 : EXPOSE 5001 8080\n",
      " ---> Running in fb69ffadaa50\n",
      "Removing intermediate container fb69ffadaa50\n",
      " ---> 2a172e08f763\n",
      "Step 27/27 : CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]\n",
      " ---> Running in 2858165d62d5\n",
      "Removing intermediate container 2858165d62d5\n",
      " ---> 31119fe7ee5d\n",
      "Successfully built 31119fe7ee5d\n",
      "Successfully tagged lvasample5d672faimodule:latest\n"
     ]
    }
   ],
   "source": [
    "!sudo docker build -t $containerImageName --file ./$isSolutionPath/Dockerfile ./$isSolutionPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If all the code cells above have successfully finished running, return to the Readme page to continue.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
