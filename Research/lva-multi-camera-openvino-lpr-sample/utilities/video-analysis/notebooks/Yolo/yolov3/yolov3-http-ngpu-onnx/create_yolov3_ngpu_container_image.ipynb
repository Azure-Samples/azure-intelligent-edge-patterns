{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Local Docker Image\n",
    "In this section, we will create an IoT Edge module, a Docker container image with an HTTP web server that has a scoring REST endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../common')\n",
    "from env_variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Web Application & Inference Server for Our ML Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/app.py\n",
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "import onnxruntime\n",
    "import json\n",
    "import logging\n",
    "import linecache\n",
    "import sys\n",
    "from score import MLModel, PrintGetExceptionDetails\n",
    "from flask import Flask, request, jsonify, Response\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "app = Flask(__name__)\n",
    "inferenceEngine = MLModel()\n",
    "\n",
    "@app.route(\"/score\", methods = ['POST'])\n",
    "def scoreRRS():\n",
    "    global inferenceEngine\n",
    "\n",
    "    try:\n",
    "        # get request as byte stream\n",
    "        reqBody = request.get_data(False)\n",
    "\n",
    "        # convert from byte stream\n",
    "        inMemFile = io.BytesIO(reqBody)\n",
    "\n",
    "        # load a sample image\n",
    "        inMemFile.seek(0)\n",
    "        fileBytes = np.asarray(bytearray(inMemFile.read()), dtype=np.uint8)\n",
    "        cvImage = cv2.imdecode(fileBytes, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Infer Image\n",
    "        detectedObjects = inferenceEngine.Score(cvImage)\n",
    "\n",
    "        if len(detectedObjects) > 0:\n",
    "            respBody = {                    \n",
    "                        \"inferences\" : detectedObjects\n",
    "                    }\n",
    "\n",
    "            respBody = json.dumps(respBody)\n",
    "            \n",
    "            logging.info(\"[AI EXT] Sending response.\")\n",
    "            return Response(respBody, status= 200, mimetype ='application/json')\n",
    "        else:\n",
    "            logging.info(\"[AI EXT] Sending empty response.\")\n",
    "            return Response(status= 204)\n",
    "\n",
    "    except:\n",
    "        PrintGetExceptionDetails()\n",
    "        return Response(response='Exception occured while processing the image.', status=500)\n",
    "    \n",
    "@app.route(\"/\")\n",
    "def healthy():\n",
    "    return \"Healthy\"\n",
    "\n",
    "# About\n",
    "@app.route('/about', methods = ['GET'])\n",
    "def about_request():\n",
    "    global inferenceEngine\n",
    "    return inferenceEngine.About()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='127.0.0.1', port=5444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, 5444 is the internal port of the webserver app that listens the requests. Next, we will map it to different ports to expose it externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/wsgi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/wsgi.py\n",
    "from app import app as application\n",
    "\n",
    "def create():\n",
    "    application.run(host='127.0.0.1', port=5444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(isSolutionPath, \"nginx\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exposed port of the web app is now 5001, while the internal one is still 5444."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/nginx/app\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/nginx/app\n",
    "server {\n",
    "    listen 5001;\n",
    "    server_name _;\n",
    " \n",
    "    location / {\n",
    "    include proxy_params;\n",
    "    proxy_pass http://127.0.0.1:5444;\n",
    "    proxy_connect_timeout 5000s;\n",
    "    proxy_read_timeout 5000s;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/gunicorn_logging.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/gunicorn_logging.conf\n",
    "\n",
    "[loggers]\n",
    "keys=root, gunicorn.error\n",
    "\n",
    "[handlers]\n",
    "keys=console\n",
    "\n",
    "[formatters]\n",
    "keys=json\n",
    "\n",
    "[logger_root]\n",
    "level=INFO\n",
    "handlers=console\n",
    "\n",
    "[logger_gunicorn.error]\n",
    "level=ERROR\n",
    "handlers=console\n",
    "propagate=0\n",
    "qualname=gunicorn.error\n",
    "\n",
    "[handler_console]\n",
    "class=StreamHandler\n",
    "formatter=json\n",
    "args=(sys.stdout, )\n",
    "\n",
    "[formatter_json]\n",
    "class=jsonlogging.JSONFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/kill_supervisor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/kill_supervisor.py\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "\n",
    "def write_stdout(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# this function is modified from the code and knowledge found here: http://supervisord.org/events.html#example-event-listener-implementation\n",
    "def main():\n",
    "    while 1:\n",
    "        write_stdout('[AI EXT] READY\\n')\n",
    "        # wait for the event on stdin that supervisord will send\n",
    "        line = sys.stdin.readline()\n",
    "        write_stdout('[AI EXT] Terminating supervisor with this event: ' + line);\n",
    "        try:\n",
    "            # supervisord writes its pid to its file from which we read it here, see supervisord.conf\n",
    "            pidfile = open('/tmp/supervisord.pid','r')\n",
    "            pid = int(pidfile.readline());\n",
    "            os.kill(pid, signal.SIGQUIT)\n",
    "        except Exception as e:\n",
    "            write_stdout('[AI EXT] Could not terminate supervisor: ' + e.strerror + '\\n')\n",
    "            write_stdout('[AI EXT] RESULT 2\\nOK')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(isSolutionPath, \"etc\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/etc/supervisord.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/etc/supervisord.conf \n",
    "[supervisord]\n",
    "logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)\n",
    "logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\n",
    "logfile_backups=10           ; (num of main logfile rotation backups;default 10)\n",
    "loglevel=info                ; (log level;default info; others: debug,warn,trace)\n",
    "pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n",
    "nodaemon=true                ; (start in foreground if true;default false)\n",
    "minfds=1024                  ; (min. avail startup file descriptors;default 1024)\n",
    "minprocs=200                 ; (min. avail process descriptors;default 200)\n",
    "\n",
    "[program:gunicorn]\n",
    "command=bash -c \"gunicorn --workers 1 -m 007 --timeout 100000 --capture-output --error-logfile - --log-level debug --log-config gunicorn_logging.conf \\\"wsgi:create()\\\"\"\n",
    "directory=/isserver\n",
    "redirect_stderr=true\n",
    "stdout_logfile =/dev/stdout\n",
    "stdout_logfile_maxbytes=0\n",
    "startretries=2\n",
    "startsecs=20\n",
    "\n",
    "[program:nginx]\n",
    "command=/usr/sbin/nginx -g \"daemon off;\"\n",
    "startretries=2\n",
    "startsecs=5\n",
    "priority=3\n",
    "\n",
    "[eventlistener:program_exit]\n",
    "command=python kill_supervisor.py\n",
    "directory=/isserver\n",
    "events=PROCESS_STATE_FATAL\n",
    "priority=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker File to Containerize the ML Solution and Web App Server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/Dockerfile\n",
    "FROM mcr.microsoft.com/azureml/onnxruntime:latest-cuda\n",
    "\n",
    "ARG WORK_DIR=/isserver\n",
    "WORKDIR ${WORK_DIR}\n",
    "\n",
    "# Copy the app file\n",
    "COPY . ${WORK_DIR}/\n",
    "COPY etc /etc\n",
    "\n",
    "# Install runit, python, nginx, and necessary python packages\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    python3-pip python3-dev libglib2.0-0 libsm6 libxext6 libxrender-dev nginx supervisor python3-setuptools \\\n",
    "    && cd /usr/local/bin \\\n",
    "    && ln -s /usr/bin/python3 python \\\n",
    "    && pip3 install --upgrade pip \\\n",
    "    && pip install numpy onnxruntime-gpu flask pillow gunicorn opencv-python json-logging-py \\\n",
    "    && apt-get clean \\\n",
    "    && apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    wget runit nginx \\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\n",
    "    && apt-get clean \\\n",
    "    && rm /etc/nginx/sites-enabled/default \\\n",
    "    && cp /isserver/nginx/app /etc/nginx/sites-available/ \\\n",
    "    && ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/\n",
    "\n",
    "EXPOSE 5001\n",
    "CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Local Docker Image\n",
    "Finally, we will create a Docker image locally. We will later host the image in a container registry like Docker Hub, Azure Container Registry, or a local registry.\n",
    "\n",
    "To run the following code snippet, you must have the pre-requisities mentioned in [the requirements page](../../../common/requirements.md). Most notably, we are running the `docker` command without `sudo`.\n",
    "\n",
    "> <span>[!WARNING]</span>\n",
    "> Please ensure that Docker is running before executing the cell below. Execution of the cell below may take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  247.9MB\n",
      "Step 1/8 : FROM mcr.microsoft.com/azureml/onnxruntime:latest-cuda\n",
      " ---> 9d181493b8a0\n",
      "Step 2/8 : ARG WORK_DIR=/isserver\n",
      " ---> Using cache\n",
      " ---> 576f492c091d\n",
      "Step 3/8 : WORKDIR ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> 877ff93bb24f\n",
      "Step 4/8 : COPY . ${WORK_DIR}/\n",
      " ---> Using cache\n",
      " ---> 01944304832a\n",
      "Step 5/8 : COPY etc /etc\n",
      " ---> Using cache\n",
      " ---> 01b2d2257d77\n",
      "Step 6/8 : RUN apt-get update && apt-get install -y --no-install-recommends     python3-pip python3-dev libglib2.0-0 libsm6 libxext6 libxrender-dev nginx supervisor python3-setuptools     && cd /usr/local/bin     && ln -s /usr/bin/python3 python     && pip3 install --upgrade pip     && pip install numpy onnxruntime-gpu flask pillow gunicorn opencv-python json-logging-py     && apt-get clean     && apt-get update && apt-get install -y --no-install-recommends     wget runit nginx     && rm -rf /var/lib/apt/lists/*     && apt-get clean     && rm /etc/nginx/sites-enabled/default     && cp /isserver/nginx/app /etc/nginx/sites-available/     && ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/\n",
      " ---> Using cache\n",
      " ---> e115b507506d\n",
      "Step 7/8 : EXPOSE 5001\n",
      " ---> Using cache\n",
      " ---> e3c93294d8ae\n",
      "Step 8/8 : CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]\n",
      " ---> Using cache\n",
      " ---> 10e690ba7551\n",
      "Successfully built 10e690ba7551\n",
      "Successfully tagged lvasample5d672faimodule:latest\n"
     ]
    }
   ],
   "source": [
    "!sudo docker build -t $containerImageName --file ./$isSolutionPath/Dockerfile ./$isSolutionPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If all the code cells above have successfully finished running, return to the Readme page to continue.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
