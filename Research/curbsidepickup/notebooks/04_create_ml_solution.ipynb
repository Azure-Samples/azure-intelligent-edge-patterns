{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Create Azure ML Solution\n",
    "In this section, we will create an inference engine wrapper, a class that will get an image data as input, analyse it and return analysis result as a json formatted data. We will be using Intel ONNX runtime for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Get global variables\n",
    "We will read the previously stored variables. We need the name of the directory that we will use to store in our ml solution files. We create a directory with the specified directory name (if not already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/mahesh/TF/curbside/notebooks/.env\n"
     ]
    }
   ],
   "source": [
    "from dotenv import set_key, get_key, find_dotenv\n",
    "envPath = find_dotenv(raise_error_if_not_found=True)\n",
    "\n",
    "mlSolutionPath = get_key(envPath, \"mlSolutionPath\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(mlSolutionPath):\n",
    "    os.mkdir(mlSolutionPath)\n",
    "print(envPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Specify ML models and create ml solution folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(mlSolutionPath):\n",
    "    os.mkdir(mlSolutionPath)\n",
    "    \n",
    "# The following values will be embedded into score.py file.  Update accordingly.\n",
    "detModelFileName = \"detector.pth\"\n",
    "redModelFileName = \"recognizer.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Create Inferenec Engine Wrapper\n",
    "HEre we create a class that will have different properties and methods to help scoring, analysing an image data. This class will also help us to specify analytics compute target such as CPU, VPU, FPGA etc. and also debugging features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp .env $mlSolutionPath/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/score.py\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import timeit as t\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import copy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "from lpdet.apis import det_and_recognize\n",
    "from lpdet.utils import Config, load_checkpoint, ProgressBar\n",
    "from lpdet.models import build_detector, build_recognizer\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AnalyticsAPI:\n",
    "    INF_STAT_OK = 0             # OK\n",
    "    INF_STAT_NOT_READY = 1      # Scoring engine is not ready\n",
    "    INF_STAT_EXCEPTION = 2      # Exception occured whiled inferencing\n",
    "\n",
    "    def __init__(self, \n",
    "                 detModelFileName=\"models/detector.pth\",\n",
    "                 recModelFileName=\"models/recognizer.pth\",\n",
    "                 detCfgFileName=\"configs/detector.py\",\n",
    "                 recCfgFileName=\"configs/recognizer.py\",\n",
    "                 workDir = \".\",\n",
    "                 probThreshold=0.5):\n",
    "        try:\n",
    "            self.initialized = False\n",
    "            self.detModelFileName = os.path.join(workDir, detModelFileName)\n",
    "            self.recModelFileName = os.path.join(workDir, recModelFileName)\n",
    "            self.detCfgFileName = os.path.join(workDir, detCfgFileName)\n",
    "            self.recCfgFileName = os.path.join(workDir, recCfgFileName)\n",
    "            self.det_cfg = Config.fromfile(self.detCfgFileName)\n",
    "            self.rec_cfg = Config.fromfile(self.recCfgFileName)\n",
    "            self.workDir = workDir\n",
    "            self.probThreshold = probThreshold\n",
    "            self.volume = \"/var/media/\" # where images write out to in container\n",
    "            self.logID = \"AnalyticsAPILogger\"\n",
    "            \n",
    "            # Load local .env file\n",
    "            load_dotenv()\n",
    "            \n",
    "            self.storageConnStr = os.getenv(\"storageConnStr\", \"\")\n",
    "            self.containerName = 'alprlva'\n",
    "            self.blob_service_client = BlobServiceClient(account_url=self.storageConnStr)\n",
    "            \n",
    "            \n",
    "            self.logger = logging.getLogger(self.logID)\n",
    "            self._lock = threading.Lock()\n",
    "            \n",
    "            self.initEngine()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.info(\"[AI EXT] Exception (AnalyticsAPI/__init__): {0}\".format(str(e)))\n",
    "            return None\n",
    "\n",
    "    def initEngine(self):\n",
    "        try:\n",
    "            with self._lock:\n",
    "                self.initialized = False\n",
    "                self.logger.info(\"[AI EXT] AnalyticsAPI init start.\")\n",
    "\n",
    "                start = t.default_timer()\n",
    "                \n",
    "                # Build network and load model                \n",
    "                self.detector = build_detector(self.det_cfg.model, test_cfg=self.det_cfg.test_cfg)\n",
    "                load_checkpoint(self.detector, self.detModelFileName)\n",
    "                self.detector.eval()\n",
    "                self.detector.to(device)\n",
    "\n",
    "                self.recognizer = build_recognizer(self.rec_cfg.model, test_cfg=self.rec_cfg.test_cfg)\n",
    "                load_checkpoint(self.recognizer, self.recModelFileName)\n",
    "                self.recognizer.eval()\n",
    "                self.recognizer.to(device)\n",
    "                \n",
    "                end = t.default_timer()\n",
    "\n",
    "                self.initialized = True\n",
    "\n",
    "                self.logger.info(\"[AI EXT] AnalyticsAPI init time: {0} ms\".format(round((end - start) * 1000, 2)))\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.info(\"[AI EXT] Exception (ScoringAPI/initModel): {0}\".format(str(e)))\n",
    "            return None\n",
    "\n",
    "    def setProbabilityThreshold(self, probThreshold=0.5):\n",
    "        self.probThreshold = probThreshold\n",
    "        self.logger.info(\"[AI EXT] (setProbabilityThreshold): {0}\".format(probThreshold))\n",
    "\n",
    "    def getProbabilityThreshold(self):\n",
    "        return self.probThreshold\n",
    "\n",
    "    def postprocess(self, boxes, scores, plates):\n",
    "        resDict = OrderedDict()\n",
    "\n",
    "        objectId = 0\n",
    "        numObjectsIdentified = len(boxes)\n",
    "        if numObjectsIdentified > 0:\n",
    "            for i in range(len(boxes)):\n",
    "                if scores[i] > self.probThreshold:\n",
    "                    box = boxes[i]\n",
    "\n",
    "                    xmin = int(box[0])\n",
    "                    ymin = int(box[1])\n",
    "                    xmax = int(box[2])\n",
    "                    ymax = int(box[3])\n",
    "\n",
    "                    resDict[objectId] = {   \"plate\": plates[i], \n",
    "                                            \"confidence\": round(float(scores[i])), \n",
    "                                            \"xmin\": xmin, \n",
    "                                            \"ymin\": ymin, \n",
    "                                            \"xmax\": xmax, \n",
    "                                            \"ymax\": ymax}\n",
    "                    objectId += 1\n",
    "\n",
    "        return resDict\n",
    "    \n",
    "    def visualize_result(self, img, reslist):\n",
    "\n",
    "        img = img.copy()\n",
    "\n",
    "        line_pairs = [(0, 1, 2, 3),\n",
    "                      (2, 3, 4, 5),\n",
    "                      (4, 5, 6, 7),\n",
    "                      (6, 7, 0, 1)]\n",
    "\n",
    "        for res in reslist:\n",
    "            r = np.round(res['corners']).astype(int)\n",
    "            for lp in line_pairs:\n",
    "                cv2.line(img, (r[lp[0]], r[lp[1]]), (r[lp[2]], r[lp[3]]), (0, 255, 0), thickness=2)\n",
    "            x1, y1 = np.min(r[0:8:2]), np.min(r[1:8:2])\n",
    "            cv2.putText(img, res['string'], (x1, y1-20), cv2.FONT_HERSHEY_COMPLEX, 3, (0, 255, 0), thickness=2)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def score(self, pilImage):\n",
    "        try:\n",
    "            with self._lock:\n",
    "                if self.initialized:\n",
    "                    \n",
    "                    start = t.default_timer()\n",
    "                    imgData = np.asarray(pilImage)\n",
    "                    # RGB to BGR (numpy to opencv)\n",
    "                    imgData = imgData[:, :, ::-1]\n",
    "                    imgSize = np.array([pilImage.size[1], pilImage.size[0]], dtype=np.float32).reshape(1, 2)\n",
    "                \n",
    "                    now = datetime.datetime.now()\n",
    "\n",
    "                    reslist = det_and_recognize(self.detector, \n",
    "                                               self.recognizer, \n",
    "                                               imgData, \n",
    "                                               det_score_thresh=0.7, \n",
    "                                               det_img_scale=(480, 360), \n",
    "                                               device=device)\n",
    "                    boxes = []\n",
    "                    scores = []\n",
    "                    plates = []\n",
    "                    for res in reslist:\n",
    "                        boxes.append(res['corners'])\n",
    "                        scores.append(res['score'])\n",
    "                        plates.append(res['string'])\n",
    "                        \n",
    "                    today = datetime.date.today()\n",
    "                    blobfolder = today.strftime(\"%Y-%m-%d\")\n",
    "                    filename = str(now).replace(' ','_').replace(':','-')+'.jpg'\n",
    "                    # Save images locally and to Blob Storage\n",
    "                    if len(reslist) > 0:\n",
    "                        imgMarked = self.visualize_result(imgData, reslist)\n",
    "\n",
    "                        # Write locally\n",
    "                        cv2.imwrite(os.path.join(self.volume, filename), \n",
    "                                    imgMarked)\n",
    "                        \n",
    "                        # To upload to Blob\n",
    "                        blob_client = self.blob_service_client.get_blob_client(container=self.containerName, \n",
    "                                                                          blob=blobfolder+'/'+filename)\n",
    "                        # Upload the local image file to Blob Storage\n",
    "                        with open(os.path.join(self.volume, filename), \"rb\") as data:\n",
    "                            blob_client.upload_blob(data)\n",
    "                        \n",
    "\n",
    "                    end = t.default_timer()\n",
    "                    infTime = round((end - start) * 1000, 2)\n",
    "\n",
    "                    resDict = self.postprocess(boxes,\n",
    "                                               scores,\n",
    "                                               plates)\n",
    "\n",
    "                    result = {  \"status\": self.INF_STAT_OK,\n",
    "                                \"time\" : infTime,\n",
    "                                \"object_count\" : len(resDict),\n",
    "                                \"result\": resDict,\n",
    "                                \"blobframe\": blobfolder+'/'+filename}\n",
    "\n",
    "                    result = json.dumps(result)\n",
    "                else:\n",
    "                    resJson = OrderedDict()\n",
    "                    resJson[0] = {\"status\": self.INF_STAT_NOT_READY}\n",
    "                    result = json.dumps(resJson)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.info(\"[AI EXT] Exception (ScoringAPI - Score): {0}\".format(str(e)))\n",
    "            resJson = OrderedDict()\n",
    "            resJson[0] = {\"status\": self.INF_STAT_EXCEPTION}\n",
    "            result = json.dumps(resJson)\n",
    "            return result\n",
    "        \n",
    "    def version(self):\n",
    "        return str(\"torch \", torch.__version__ + \" - project v1.0\")\n",
    "\n",
    "    def about(self):\n",
    "        aboutString = \"Engine initialized: {0}<br>ProbThreshold: {1}\".format(self.initialized, self.probThreshold)\n",
    "        return aboutString        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score method of the above InferenceEngine class will return the result as Json formated string.  \n",
    "\n",
    "Result will be one of the below three Json string:  \n",
    "\n",
    "1) {\"status\": 1}  \n",
    "Above result means the Inference Engine is not ready for inferencing...\n",
    "\n",
    "2) {\"status\": 2}  \n",
    "Above result means an exception occured while scoring. Details are in the logs...\n",
    "\n",
    "3) Third option is below where status equals to 0. So before you you process the result, you can check the status to see if it contains valid detection data, status code.\n",
    "\n",
    "Fields below are self descriptive and you can refere to above source code for details.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "   \"status\": 0,\n",
    "   \"time\": 45.6475,\n",
    "   \"object_count\": 100,\n",
    "   \"result\": {\n",
    "      \"0\": {\n",
    "         \"label\": 1,\n",
    "         \"confidence\": 1,\n",
    "         \"xmin\": 304,\n",
    "         \"ymin\": 169,\n",
    "         \"xmax\": 398,\n",
    "         \"ymax\": 436\n",
    "      },\n",
    "      \"1\": {\n",
    "         \"label\": 2,\n",
    "         \"confidence\": 0.99,\n",
    "         \"xmin\": 474,\n",
    "         \"ymin\": 201,\n",
    "         \"xmax\": 586,\n",
    "         \"ymax\": 485\n",
    "      },\n",
    "\n",
    "        ...\n",
    "\n",
    "      \"100\": {\n",
    "         \"label\": 5,\n",
    "         \"confidence\": 0.53,\n",
    "         \"xmin\": 385,\n",
    "         \"ymin\": 140,\n",
    "         \"xmax\": 421,\n",
    "         \"ymax\": 246\n",
    "      }\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
