{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Build Iot Edge Module - Docker Image\n",
    "If not already, you should run first jupyter notebook (01_setup_environment.ipynb) in this sample to set the global variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Get global variables\n",
    "We will read the previously stored variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import set_key, get_key, find_dotenv\n",
    "envPath = find_dotenv(raise_error_if_not_found=True)\n",
    "\n",
    "mlSolutionPath = get_key(envPath, \"mlSolutionPath\")\n",
    "containerImageName = get_key(envPath, \"containerImageName\")\n",
    "localContainerRegServiceName = get_key(envPath, \"localContainerRegServiceName\")\n",
    "containerRegServiceName = get_key(envPath, \"containerRegServiceName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teamlvargaimodule\n",
      "teamlvacontainerregistry:2\n"
     ]
    }
   ],
   "source": [
    "print(containerImageName)\n",
    "print(localContainerRegServiceName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove possible python cache from previous local tests\n",
    "!rm -rf $mlSolutionPath/__pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create Web Application & Server for our ml solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/app.py\n",
    "\n",
    "from flask import Flask, request, Response\n",
    "from PIL import Image\n",
    "import logging\n",
    "import json\n",
    "import io\n",
    "from score import AnalyticsAPI\n",
    "\n",
    "app = Flask(__name__)\n",
    "analyticsAPI = AnalyticsAPI(workDir=\".\")\n",
    "\n",
    "@app.route(\"/score\", methods = ['POST'])\n",
    "def scoreRRS():\n",
    "    global analyticsAPI\n",
    "    #analyticsAPI.logger.info(\"[AI EXT] Received scoring request. Header: {0}\".format(json.dumps(dict(request.headers))))\n",
    "\n",
    "    try:    \n",
    "        if request.headers['Content-Type'] != 'image/jpeg':\n",
    "            analyticsAPI.logger.info(\"[AI EXT] Non JPEG content sent. Exiting the scoring event...\")\n",
    "            return Response(json.dumps({}), status= 415, mimetype ='application/json')\n",
    "\n",
    "        # get request as byte stream\n",
    "        reqBody = request.get_data(False)\n",
    "\n",
    "        # convert from byte stream\n",
    "        inMemFile = io.BytesIO(reqBody)\n",
    "\n",
    "        # load a sample image\n",
    "        pilImage = Image.open(inMemFile)\n",
    "\n",
    "        # call scoring function\n",
    "        result = analyticsAPI.score(pilImage)            \n",
    "\n",
    "        analyticsAPI.logger.info(\"[AI EXT] Sending response.\")\n",
    "        return Response(result, status= 200, mimetype ='application/json')\n",
    "\n",
    "    except Exception as e:\n",
    "        analyticsAPI.logger.info(\"[AI EXT] Exception (scoreRRS): {0}\".format(str(e)))\n",
    "        return Response(json.dumps({}), status= 200, mimetype ='application/json')   \n",
    "    \n",
    "@app.route(\"/\")\n",
    "def healthy():\n",
    "    return \"Healthy\"\n",
    "\n",
    "# Version\n",
    "@app.route('/version', methods = ['GET'])\n",
    "def version_request():\n",
    "    global analyticsAPI\n",
    "    return analyticsAPI.version()\n",
    "\n",
    "# About\n",
    "@app.route('/about', methods = ['GET'])\n",
    "def about_request():\n",
    "    global analyticsAPI\n",
    "    return analyticsAPI.about()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while not analyticsAPI.initialized:\n",
    "        logger.info(\"[AI EXT] Waiting AI module to be initialized. (app.py)\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    app.run(host='0.0.0.0', port=5444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to make any change here but 5444 is the internal port of the webserver app that listens the requests. Later we will map it to different port to expose to external (see next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/wsgi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/wsgi.py\n",
    "from app import app as application\n",
    "\n",
    "def create():\n",
    "    print(\"[AI EXT] Initialising lva ai extension web app\")\n",
    "    application.run(host='127.0.0.1', port=5444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(mlSolutionPath, \"nginx\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exposed port of the web app is now 5001 (while the internal one is 5444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/nginx/app\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/nginx/app\n",
    "server {\n",
    "    listen 5001;\n",
    "    server_name _;\n",
    " \n",
    "    location / {\n",
    "    include proxy_params;\n",
    "    proxy_pass http://127.0.0.1:5444;\n",
    "    proxy_connect_timeout 5000s;\n",
    "    proxy_read_timeout 5000s;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/gunicorn_logging.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/gunicorn_logging.conf\n",
    "\n",
    "[loggers]\n",
    "keys=root, gunicorn.error\n",
    "\n",
    "[handlers]\n",
    "keys=console\n",
    "\n",
    "[formatters]\n",
    "keys=json\n",
    "\n",
    "[logger_root]\n",
    "level=INFO\n",
    "handlers=console\n",
    "\n",
    "[logger_gunicorn.error]\n",
    "level=ERROR\n",
    "handlers=console\n",
    "propagate=0\n",
    "qualname=gunicorn.error\n",
    "\n",
    "[handler_console]\n",
    "class=StreamHandler\n",
    "formatter=json\n",
    "args=(sys.stdout, )\n",
    "\n",
    "[formatter_json]\n",
    "class=jsonlogging.JSONFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/kill_supervisor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/kill_supervisor.py\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "\n",
    "def write_stdout(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# this function is modified from the code and knowledge found here: http://supervisord.org/events.html#example-event-listener-implementation\n",
    "def main():\n",
    "    while 1:\n",
    "        write_stdout('[AI EXT] READY\\n')\n",
    "        # wait for the event on stdin that supervisord will send\n",
    "        line = sys.stdin.readline()\n",
    "        write_stdout('[AI EXT] Killing supervisor with this event: ' + line);\n",
    "        try:\n",
    "            # supervisord writes its pid to its file from which we read it here, see supervisord.conf\n",
    "            pidfile = open('/tmp/supervisord.pid','r')\n",
    "            pid = int(pidfile.readline());\n",
    "            os.kill(pid, signal.SIGQUIT)\n",
    "        except Exception as e:\n",
    "            write_stdout('[AI EXT] Could not kill supervisor: ' + e.strerror + '\\n')\n",
    "            write_stdout('[AI EXT] RESULT 2\\nOK')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(mlSolutionPath, \"etc\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/etc/supervisord.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/etc/supervisord.conf \n",
    "[supervisord]\n",
    "logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)\n",
    "logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\n",
    "logfile_backups=10           ; (num of main logfile rotation backups;default 10)\n",
    "loglevel=info                ; (log level;default info; others: debug,warn,trace)\n",
    "pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n",
    "nodaemon=true                ; (start in foreground if true;default false)\n",
    "minfds=1024                  ; (min. avail startup file descriptors;default 1024)\n",
    "minprocs=200                 ; (min. avail process descriptors;default 200)\n",
    "\n",
    "[program:gunicorn]\n",
    "command=bash -c \"gunicorn --workers 1 -m 007 --timeout 100000 --capture-output --error-logfile - --log-level debug --log-config gunicorn_logging.conf \\\"wsgi:create()\\\"\"\n",
    "directory=/code\n",
    "redirect_stderr=true\n",
    "stdout_logfile =/dev/stdout\n",
    "stdout_logfile_maxbytes=0\n",
    "startretries=2\n",
    "startsecs=20\n",
    "\n",
    "[program:nginx]\n",
    "command=/usr/sbin/nginx -g \"daemon off;\"\n",
    "startretries=2\n",
    "startsecs=5\n",
    "priority=3\n",
    "\n",
    "[eventlistener:program_exit]\n",
    "command=python kill_supervisor.py\n",
    "directory=/code\n",
    "events=PROCESS_STATE_FATAL\n",
    "priority=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/requirements.txt\n",
    "pillow<7.0.0\n",
    "click==6.7\n",
    "configparser==3.5.0\n",
    "Flask==0.12.2\n",
    "gunicorn==19.6.0\n",
    "json-logging-py==0.2\n",
    "MarkupSafe==1.0\n",
    "olefile==0.44\n",
    "requests==2.12.3\n",
    "six==1.13.0\n",
    "opencv-python==4.1.1.26\n",
    "imgaug==0.3.0\n",
    "torch==1.2\n",
    "cython==0.29.14\n",
    "addict==2.2.1\n",
    "azure-storage-blob==12.3.0\n",
    "python-dotenv==0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Docker File to containerize ml solution and web app server\n",
    "\n",
    "Instead of installing latest ONNX Runtime python package, we will build a base docker image with ONNX Runtime to be compiled from latest source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $mlSolutionPath/install_onnxruntime.sh\n",
    "# #!/bin/bash\n",
    "# DEBIAN_FRONTEND=noninteractive\n",
    "# apt-get update && apt-get install -y --no-install-recommends \\\n",
    "#         git \\\n",
    "#         wget \\\n",
    "#         zip \\\n",
    "#         ca-certificates \\\n",
    "#         build-essential \\\n",
    "#         curl \\\n",
    "#         libcurl4-openssl-dev \\\n",
    "#         libssl-dev \\\n",
    "#         python3-dev \\\n",
    "#         sudo \\\n",
    "#         nano\n",
    "\n",
    "# cd ${WORK_DIR}\n",
    "\n",
    "# # Dependencies: conda\n",
    "# wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate\n",
    "# /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda\n",
    "# /opt/miniconda/bin/conda clean -ya\n",
    "\n",
    "# pip install numpy\n",
    "# rm -rf /opt/miniconda/pkgs\n",
    "\n",
    "# # Dependencies: cmake\n",
    "# wget --quiet https://github.com/Kitware/CMake/releases/download/v3.14.3/cmake-3.14.3-Linux-x86_64.tar.gz\n",
    "# tar zxf cmake-3.14.3-Linux-x86_64.tar.gz\n",
    "\n",
    "# # clone latest ONNX runtime source for compile\n",
    "# git clone --single-branch --branch ${ONNXRUNTIME_SERVER_BRANCH} --recursive ${ONNXRUNTIME_REPO} onnxruntime\n",
    "\n",
    "# cd ${WORK_DIR}/onnxruntime\n",
    "# /bin/bash  ./build.sh --config Release --build_wheel --update --build --parallel --cmake_extra_defines ONNXRUNTIME_VERSION=$(cat ./VERSION_NUMBER)\n",
    "\n",
    "# pip install ${WORK_DIR}/onnxruntime/build/Linux/Release/dist/*.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/install_mlsolution.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/install_mlsolution.sh\n",
    "#!/bin/bash\n",
    "DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        nginx \\\n",
    "        supervisor\n",
    "\n",
    "rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "rm /etc/nginx/sites-enabled/default && \\\n",
    "    cp /code/nginx/app /etc/nginx/sites-available/ && \\\n",
    "    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ && \\\n",
    "    pip install -r /code/requirements.txt && \\       \n",
    "    /opt/conda/bin/conda clean -ya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $mlSolutionPath/Dockerfile\n",
    "# FROM ubuntu:18.04\n",
    "# MAINTAINER Micheleen Harris\n",
    "\n",
    "# ARG ONNXRUNTIME_REPO=https://github.com/Microsoft/onnxruntime\n",
    "# ARG ONNXRUNTIME_SERVER_BRANCH=master\n",
    "# ARG WORK_DIR=/code\n",
    "\n",
    "# RUN mkdir /code\n",
    "\n",
    "# WORKDIR ${WORK_DIR}\n",
    "# ENV PATH /opt/miniconda/bin:${WORK_DIR}/cmake-3.14.3-Linux-x86_64/bin:${PATH}\n",
    "# ENV WORK_DIR ${WORK_DIR}\n",
    "\n",
    "# RUN apt-get update &&\\\n",
    "#     apt-get install -y bash\n",
    "\n",
    "# # Prepare onnxruntime repository & build onnxruntime\n",
    "# COPY install_onnxruntime.sh ${WORK_DIR}\n",
    "\n",
    "# RUN cd ${WORK_DIR} && \\\n",
    "#     /bin/bash ${WORK_DIR}/install_onnxruntime.sh && \\\n",
    "#     rm -rf *\n",
    "\n",
    "# ADD . /code/\n",
    "# ADD etc /etc\n",
    "\n",
    "# # Setup ml solution\n",
    "# COPY install_mlsolution.sh ${WORK_DIR}\n",
    "\n",
    "# RUN cd ${WORK_DIR} && \\\n",
    "#     /bin/bash ${WORK_DIR}/install_mlsolution.sh\n",
    "\n",
    "# EXPOSE 5001\n",
    "# CMD [\"supervisord\", \"-c\", \"/code/etc/supervisord.conf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/alpr/lva_ai_solution/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $mlSolutionPath/Dockerfile\n",
    "FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\n",
    "MAINTAINER Micheleen Harris\n",
    "\n",
    "ARG WORK_DIR=/code\n",
    "\n",
    "ENV LANG=C.UTF-8 LC_ALL=C.UTF-8\n",
    "ENV PATH /opt/conda/bin:$PATH\n",
    "ENV TORCH_CUDA_ARCH_LIST=Volta;Turing;Kepler+Tesla\n",
    "\n",
    "RUN mkdir /code\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "         build-essential \\\n",
    "         cmake \\\n",
    "         git \\\n",
    "         curl \\\n",
    "         wget \\\n",
    "         ca-certificates \\\n",
    "         libjpeg-dev \\\n",
    "         libpng-dev \\\n",
    "         swig \\\n",
    "         libgtk2.0-dev && \\\n",
    "     rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "WORKDIR ${WORK_DIR}\n",
    "\n",
    "RUN wget --quiet https://repo.continuum.io/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh && \\\n",
    "    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/conda && \\\n",
    "    rm ${WORK_DIR}/miniconda.sh && \\\n",
    "    ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \\\n",
    "    echo \". /opt/conda/etc/profile.d/conda.sh\" >> ~/.bashrc && \\\n",
    "    echo \"conda activate base\" >> ~/.bashrc\n",
    "\n",
    "# This must be done before pip so that requirements.txt is available\n",
    "WORKDIR ${WORK_DIR}\n",
    "COPY . .\n",
    "\n",
    "# Install python requirements for project\n",
    "RUN CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "# Compile ops\n",
    "RUN chmod +x compile_ops.sh && \\\n",
    "    CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\n",
    "    ./compile_ops.sh\n",
    "\n",
    "RUN chmod -R a+w .\n",
    "\n",
    "ADD . /code/\n",
    "ADD etc /etc\n",
    "\n",
    "# Setup ml solution\n",
    "COPY install_mlsolution.sh ${WORK_DIR}\n",
    "\n",
    "RUN cd ${WORK_DIR} && \\\n",
    "    /bin/bash ${WORK_DIR}/install_mlsolution.sh\n",
    "\n",
    "EXPOSE 5001\n",
    "CMD [\"supervisord\", \"-c\", \"/code/etc/supervisord.conf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Create Local Docker Image\n",
    "We create the image locally, without hostring it yet in a container registry like docker.com or ACR or local registry\n",
    "\n",
    "Dont forget that you need to have pre-requisities mentioned in section 1... We are running docker command without \"sudo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teamlvargaimodule\n",
      "../src/alpr/lva_ai_solution\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$containerImageName\" \"$mlSolutionPath\"\n",
    "echo \"$1\"\n",
    "echo \"$2\"\n",
    "#docker build -t $1 --file ./$2/Dockerfile ./$2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Host the local docker image in local docker registry\n",
    "We will be hosting the docker image on a local registry which is either on the edge device, or on the development device which is on the same network as the edge device. We prefer this method only at the development stage as:\n",
    "- Docker image size ~ 800 Mb\n",
    "- Hosting image on Azure Container Registry (ACR) means each time we compile the image, we will send it, push it to cloud and then pull all the way back to edge device. In case of low Internet connection bandwidth, this may be an issue or time consuming operation.\n",
    "- There is an option where you can send smaller size DockerFile and dependencies to ACR and compile there. But in this case, 1) again you need to pull back the 1-2 GB size image to edge device 2) Even in case of a small code change in the last layers of the docker image, ACR will re-compile every layer from scratc which may take additional ~40 minutes...\n",
    "\n",
    "So for above reasons, at the development and testing stages, we will use local container registry. Below is very simple steps to create your own docker registry by running the following bash commands manually:  \n",
    "\n",
    "<span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!Important] </span>  \n",
    "In the below instructions we use **\"mkregistry:55000\"** as static text which is stored in the variable **localContainerRegServiceName**. Value of this variable set in the first step/section of this sample. You can change this value to anything else but in case, you should update the commands in this static text with the new value.\n",
    "\n",
    "\n",
    "1) Create a folder to host local docker registry configuration folder (i.e. we name it \"mkregistry\").\n",
    "```shell\n",
    "mkdir mkregistry\n",
    "```\n",
    "2) Inside the \"mkregistry\" directory, create a congiguration file named \"docker-compose.yml\" with the following content in:\n",
    "\n",
    "```yml\n",
    "version: '3.0'\n",
    " \n",
    "services:\n",
    "  mkregistry:\n",
    "    image: registry:latest\n",
    "    container_name: mkregistry\n",
    "    volumes:\n",
    "      - registry:/var/lib/registry\n",
    "    ports:\n",
    "      - \"55000:5000\"\n",
    "    restart: unless-stopped\n",
    "volumes:\n",
    "  registry:\n",
    "```\n",
    "\n",
    "3) Install \"docker-compose\" tool. Run the following shell command to install it.  \n",
    "```shell\n",
    "sudo apt-get -y install docker-compose\n",
    "```\n",
    "\n",
    "4) Run the following command inside the \"mkregistry\" directory with \"docker-compose.yml\" file in it.  \n",
    "```shell\n",
    "sudo docker-compose up -d\n",
    "```\n",
    "\n",
    "<span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!Important] </span>  \n",
    "> When you restart the machine etc. you need to re-run above command to start the local regisrty service (you can search for methods to run it as auto run service)\n",
    "when you type \"docker container ls\" shell command. You should see the following line to confirm that the service is running, you can validate with below next step.\n",
    "```\n",
    "f63fd9492a03        registry:latest                              \"/entrypoint.sh /etcâ€¦\"    20 hours ago        Up 19 hours         0.0.0.0:55000->5000/tcp                                                mkregistry\n",
    "m\n",
    "```\n",
    "\n",
    "5) In your Internet explorer - web browser, you browse to following address to see the images in your local repository (initially it is empty...)  \n",
    "```\n",
    "http://localhost:55000/v2/_catalog\n",
    "```\n",
    "\n",
    "6) Edit \"hosts\" file to assign a name resolution setting for local docker registry. First open the \"/etc/hosts\" file with your favorite text editor:  \n",
    "```shell\n",
    "sudo nano /etc/hosts\n",
    "```\n",
    "\n",
    "than add the following line under \"127.0.0.0 localhost\"  \n",
    "\n",
    "```shell\n",
    "127.0.0.1\tmkregistry\n",
    "```\n",
    "\n",
    "so the content of \"/etc/hosts\" file should look like something:  \n",
    "```shell\n",
    "127.0.0.1\tlocalhost\n",
    "127.0.0.1\tmkregistry\n",
    "127.0.1.1\tmknuc01\n",
    "\n",
    "# The following lines are desirable for IPv6 capable hosts\n",
    "::1     ip6-localhost ip6-loopback\n",
    "fe00::0 ip6-localnet\n",
    "ff00::0 ip6-mcastprefix\n",
    "ff02::1 ip6-allnodes\n",
    "ff02::2 ip6-allrouters\n",
    "```\n",
    "\n",
    "thats all, now we have local docker registy with following addres: mkregistry:55000  \n",
    "\n",
    "With following code, we will tag our local image and pull it into our local docker registry. So when we deploy our IotEdge manifest to IoT Hub, the edge devices will pull it from this registry. In production stage, we will host the image in ACR, more centric and managable store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$containerImageName\" \"$localContainerRegServiceName\"\n",
    "docker tag $1 $2/$1\n",
    "docker push $2/$1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(containerImageName)\n",
    "print(localContainerRegServiceName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!Important] </span>  \n",
    "If you are in production, you can push the image into ACR (and not local repository). Uncomment and use the following command to push it into ACR. Also dont forget to uncomment another line in the next step/section of the sample where you deploy the image into edge devices. So the edge device will pull modules from the ACR and not from local registry...\n",
    "\n",
    "<span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!Warning] </span>  \n",
    "Execution of following cell may take up to ~40 - 45 minutes to complete! It will be using Azure Constainer Registry (ACR) to compile a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!az acr build --image $containerImageName --registry $containerRegServiceName --file ./$mlSolutionPath/Dockerfile ./$mlSolutionPath"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
