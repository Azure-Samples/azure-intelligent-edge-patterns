{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Segmenation on Azure Stack Hub Clusters\n",
    "\n",
    "For this tutorial, we will fine tune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it  to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "\n",
    "You will use [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define two pipeline steps: a data process step which split data into training and testing, and training step which trains and evaluates the model.  The trained model then registered to your AML workspace.\n",
    "\n",
    "\n",
    "After the model is registered, you then deploy the model for serving or testing. You will deploy the model to different compute platform: 1) Azure Kubernetes Cluster (AKS), 2) your local computer\n",
    "\n",
    "This is a notebook about using ASH storage and ASH cluster (ARC compute) for training and serving, please make sure the following prerequisites are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*    [ A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC](https://github.com/lisongshan007/azure-intelligent-edge-patterns/tree/master/Research/object-segmentation-on-azure-stack/AML-ARC-Compute.md).\n",
    "     \n",
    "\n",
    "*     [Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account](https://github.com/lisongshan007/azure-intelligent-edge-patterns/tree/master/Research/object-segmentation-on-azure-stack/Train-AzureArc.md) \n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace,Environment, Experiment, Datastore\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "\n",
    "In preparation stage, you will create a AML workspace first, then\n",
    "\n",
    "*  Create a compute target for AML workspace by attaching the cluster deployed on Azure Stack Hub (ASH)\n",
    "\n",
    "*  Create a datastore for AML workspace backed by storage account deployed on ASH.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Compute Target by attaching cluster deployed on ASH\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>\n",
    "\n",
    "attach_name is the attached name for your ASH cluster you setup in [this step](https://github.com/lisongshan007/azure-intelligent-edge-patterns/tree/master/Research/object-segmentation-on-azure-stack/AML-ARC-Compute.md)\n",
    "\n",
    "Attaching ASH cluster the first time may take 7 minutes. It will be much faster after first attachment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.kubernetescompute import KubernetesCompute\n",
    "\n",
    "attach_name = \"<NAME_OF_AML_ATTACHED_COMPUTE_OF_YOUR_ASH_CLUSTER>\"\n",
    "arcK_target = KubernetesCompute(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datastore for AML workspace backed by storage account deployed on ASH\n",
    "\n",
    "Here is the [instruction](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "After downloading and extracting the zip file from [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) to your local machine, you will have the following folder structure:\n",
    "\n",
    "<pre>\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "</pre>\n",
    "\n",
    "\n",
    "\"datastore_name\" is the name of the datastore you setup in [this step](https://github.com/lisongshan007/azure-intelligent-edge-patterns/tree/master/Research/object-segmentation-on-azure-stack/Train-AzureArc.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "dataset_name = \"pennfudan_ds\"\n",
    "datastore_name = \"<NAME_OF_ASH_HOSTED_AML_DATASTORE>\"\n",
    "\n",
    "if dataset_name not  in ws.datasets:\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "    src_dir, target_path = 'PennFudanPed', 'PennFudanPed'\n",
    "    datastore.upload(src_dir, target_path, overwrite=True)\n",
    "\n",
    "    # register data uploaded as AML dataset\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    pd_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    pd_ds.register(ws, dataset_name, \"for Pedestrian Detection and Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training-Test split data process Step\n",
    "\n",
    "For this pipeline run, you will use two pipeline steps.  The first step is to split dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create run_config first\n",
    "\n",
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = arcK_target\n",
    "aml_run_config.environment = env\n",
    "\n",
    "source_directory = './aml_src'\n",
    "\n",
    "# add a data process step\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "dest = (datastore, None)\n",
    "\n",
    "train_split_data = OutputFileDatasetConfig(name=\"train_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "test_split_data = OutputFileDatasetConfig(name=\"test_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "split_step = PythonScriptStep(\n",
    "    name=\"Train Test Split\",\n",
    "    script_name=\"obj_segment_step_data_process.py\",\n",
    "    arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\n",
    "               \"--train-split\", train_split_data, \"--test-split\", test_split_data,\n",
    "               \"--test-size\", 50],\n",
    "    compute_target=arcK_target,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=source_directory,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "        name=\"training_step\",\n",
    "        script_name=\"obj_segment_step_training.py\",\n",
    "        arguments=[\n",
    "            \"--train-split\", train_split_data.as_input(), \"--test-split\", test_split_data.as_input(),\n",
    "            '--epochs', 1,  # 80\n",
    "        ],\n",
    "\n",
    "        compute_target=arcK_target,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=source_directory,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment and Submit Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'obj_seg_step'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "pipeline_steps = [train_step]\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "pipeline_run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "Note: Here we saved and register two models. The model saved at \"'outputs/obj_segmentation.pkl'\" is registered as   'obj_seg_model_aml'.  It contains both model parameters and network which are used by AML deployment and serving.  Model 'obj_seg_model_kf_torch' contains only parameter values which maybe used for KFServing as shown in [this notebook](object_segmentation_kfserving.ipynb) If you are not interesting in KFServing, you can safely ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "\n",
    "model_name = 'obj_seg_model_aml' # model for AML serving\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/obj_segmentation.pkl')\n",
    "\n",
    "model_name = 'obj_seg_model_kf_torch' # model for KFServing using pytorchserver\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model\n",
    "\n",
    "Here we give a few examples of deploy the model to different siturations:\n",
    "\n",
    "*   Deploy to Azure Kubernetes Cluster\n",
    "*   [Deploy to local computer as a Local Docker Web Service](#local_deployment)\n",
    "*   [Deploy to ASH clusters with KFServing](object_segmentation_kfserving.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Azure Kubernetes Cluster \n",
    "\n",
    "There are two steps:\n",
    "\n",
    "1.   Create (or use existing) a AKS cluster for serving the model\n",
    "\n",
    "2.   Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision the AKS Cluster\n",
    "\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it. It may take 5 mins to create a new AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "    \n",
    "print(\"using compute target: \", aks_target.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "env.inferencing_stack_version='latest'\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "deployed_model =   model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "service_name = 'objservice10'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Service\n",
    "\n",
    "You can \n",
    "\n",
    "*   test service directly with the service object you deployed.\n",
    "\n",
    "*   test use the restful end point.\n",
    "\n",
    "For testing purpose, you may take the first image FudanPed00001.png as example. This image looks like this ![fishy](FudanPed00001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nums = [\"00001\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img.show(\"input_image\")\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "inputs = json.dumps({\"instances\": image_np_list})\n",
    "resp = service.run(inputs)\n",
    "predicts = resp[\"predictions\"]\n",
    "\n",
    "for instance_pred in predicts:\n",
    "    print(\"labels\", instance_pred[\"labels\"])\n",
    "    print(\"boxes\", instance_pred[\"boxes\"])\n",
    "    print(\"scores\", instance_pred[\"scores\"])\n",
    "    \n",
    "    image_data = instance_pred[\"masks\"]\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a  function to call the url end point\n",
    "\n",
    "Creae a simple help function to wrap the restful endpoint call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def service_infer(url, body, api_key):\n",
    "    headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "\n",
    "        result = response.read()\n",
    "        return result\n",
    "\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using restful end point\n",
    "\n",
    "Go to EndPonts section of your azure machine learning workspace, you will find the service you deployed. Click the service name, then click \"consume\", you will see the restful end point (the uri) and api key of your service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = \"<endpoints url>\"\n",
    "api_key = '<api_key>'  # Replace this with the API key for the web service\n",
    "\n",
    "img_nums = [\"00001\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img.show(\"input_image\")\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "request = {\"instances\": image_np_list}\n",
    "inputs = json.dumps(request)\n",
    "\n",
    "body = str.encode(inputs)\n",
    "resp = service_infer(url, body, api_key)\n",
    "p_obj = json.loads(resp)\n",
    "\n",
    "predicts = p_obj[\"predictions\"]\n",
    "\n",
    "for instance_pred in predicts:\n",
    "    print(\"labels\", instance_pred[\"labels\"])\n",
    "    print(\"boxes\", instance_pred[\"boxes\"])\n",
    "    print(\"scores\", instance_pred[\"scores\"])\n",
    "    \n",
    "    image_data = instance_pred[\"masks\"]\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='local_deployment'></a>\n",
    "\n",
    "## Deploy model to local computer as a Local Docker Web Service\n",
    "\n",
    "Make sure you have Docker installed and running. Note that the service creation can take few minutes.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The Docker image runs as a Linux container. If you are running Docker for Windows, you need to ensure the Linux Engine is running.\n",
    "PowerShell command to switch to Linux engine is\n",
    "\n",
    "<pre>\n",
    "& 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchLinuxEngine\n",
    "</pre>\n",
    "\n",
    "Also, you need to login to az and acr:\n",
    "<pre>\n",
    "az login\n",
    "az acr login --name your_acr_name\n",
    "</pre>\n",
    "For more details, please see [this notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# This is optional, if not provided Docker will choose a random unused port.\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "deployed_model = \"obj_seg_model_aml\" # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "local_service = Model.deploy(ws, \"localtest\", [model], inference_config, deployment_config)\n",
    "\n",
    "local_service.wait_for_deployment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Local service port: {}'.format(local_service.port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status and Get Container Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test local deployment\n",
    "\n",
    "You can use local_service.run to test your deployment as shown in  case of deployment to Kubernetes Cluster. You can use end point to test your inference service as well.  In this local deployment, the endpoint is http://localhost:{port}/score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    " [Deploy to ASH clusters with KFServing](object_segmentation_kfserving.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
