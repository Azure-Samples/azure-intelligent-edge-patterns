{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Segmenation on Azure Stack Hub Clusters\n",
    "\n",
    "For this tutorial, we will fine tune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it  to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "\n",
    "You will use [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define two pipeline steps: a data process step which split data into training and testing, and training step which trains and evaluates the model.  The trained model then registered to your AML workspace.\n",
    "\n",
    "\n",
    "After the model is registered, you then deploy the model for serving or testing. You will deploy the model to different compute platform: 1) Azure Kubernetes Cluster (AKS), 2) your local computer, 3) KF Serving to ASH cluster (todo)\n",
    "\n",
    "This is a notebook about using ASH storage and ASH cluster (ARC compute) for training and serving, please make sure the following prerequisites are met.\n",
    "\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "*  A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "  \n",
    "    \n",
    "    1.  To create a Kubernetes cluster on Azure Stack Hub, please see \n",
    "            [here](https://docs.microsoft.com/en-us/azure-stack/user/azure-stack-solution-template-kubernetes-deploy?view=azs-2008)\n",
    "\n",
    "\n",
    "    2.  Connect  Azure Stack Hubâ€™s Kubernetes cluster to  Azure via Azure ARC\n",
    "    \n",
    "           This notebook  requires az extension k8s-extension >= 0.1 and connectedk8s >= 0.3.2 installed on the cluster's master node. \n",
    "           Current [public preview released version for connectedk8s](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster) is 0.2.8,\n",
    "           so you need to install [private preview](https://github.com/Azure/azure-arc-kubernetes-preview/blob/master/docs/k8s-extensions.md)\n",
    "  \n",
    "\n",
    "*  A storage account deployed on Azure Stack Hub.\n",
    "\n",
    "\n",
    "*  Setup Azure Machine Learning workspace on Azure.\n",
    "\n",
    "\n",
    "     please make sure the following\n",
    "     [Prerequisites](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?     tabs=python#prerequisites) are\n",
    "     met (we recommend using the Python SDK when communicating with Azure Machine Learning so make sure the SDK is properly installed). \n",
    "     We strongly recommend learning more about [the innerworkings and concepts in Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture) before continuing with the rest of this article (optional)\n",
    "\n",
    "\n",
    "*  Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "    If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace,Environment, Experiment, Datastore\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "\n",
    "In preparation stage, you will create a AML workspace first, then\n",
    "\n",
    "*  Create a compute target for AML workspace by attaching the cluster deployed on Azure Stack Hub (ASH)\n",
    "\n",
    "*  Create a datastore for AML workspace backed by storage account deployed on ASH.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Compute Target\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "\n",
    "resource_id = \"<resource_id>\"\n",
    "attach_name = \"slw4pipeline11\"\n",
    "script = 'obj_segament.py'\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(resource_id=resource_id)\n",
    "\n",
    "attach_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "\n",
    "attach_result.wait_for_completion(show_output=True)\n",
    "\n",
    "print(attach_result)\n",
    "\n",
    "compute_target = ws.compute_targets[attach_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datastore for AML workspace backed by storage account deployed on ASH\n",
    "\n",
    "Here is the [instruction](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "After downloading and extracting the zip file from [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) to your local machine, you will have the following folder structure:\n",
    "\n",
    "<pre>\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "</pre>\n",
    "\n",
    "### todo: reference: how to create an data store with ash storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "dataset_name = \"pennfudan_2\"\n",
    "if dataset_name not  in ws.datasets:\n",
    "    \n",
    "    datastore_name = \"ashstore\"\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    datastore.upload('PennFudanPed', 'PennFudanPed')\n",
    "\n",
    "    datastore_paths = [(datastore, 'PennFudanPed')]\n",
    "\n",
    "    pd_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    pd_ds.register(ws, dataset_name, \"for Pedestrian Detection and Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training-Test split data process Step\n",
    "\n",
    "For this pipeline run, you will use two pipeline steps.  The first step is to split dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create run_config first\n",
    "\n",
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute_target.name\n",
    "aml_run_config.environment = env\n",
    "\n",
    "source_directory = './aml_src'\n",
    "\n",
    "\n",
    "# add a data process step\n",
    "\n",
    "dataset = ws.datasets[dataset_name]\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "dest = (datastore, None)\n",
    "\n",
    "train_split_data = OutputFileDatasetConfig(name=\"train_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "test_split_data = OutputFileDatasetConfig(name=\"test_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "\n",
    "split_step = PythonScriptStep(\n",
    "    name=\"Train Test Split\",\n",
    "    script_name=\"obj_segment_step_data_process.py\",\n",
    "    arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\n",
    "               \"--train-split\", train_split_data, \"--test-split\", test_split_data,\n",
    "               \"--test-size\", 50],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=source_directory,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "        name=\"training_step\",\n",
    "        script_name=\"obj_segment_step_training.py\",\n",
    "        arguments=[\n",
    "            \"--train-split\", train_split_data.as_input(), \"--test-split\", test_split_data.as_input(),\n",
    "            '--epochs', 1,  # 80\n",
    "        ],\n",
    "\n",
    "        compute_target=compute_target,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=source_directory,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment and Submit Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'obj_seg_step'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "pipeline_steps = [train_step]\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "pipeline_run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "\n",
    "model_name = 'obj_seg_model_2'\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/obj_segmentation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model\n",
    "\n",
    "Here we give a few examples of deploy the model to different siturations:\n",
    "\n",
    "*   Deploy to Azure Kubernetes Cluster\n",
    "*   Deploy to local computer as a Local Docker Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to Azure Kubernetes Cluster \n",
    "\n",
    "There are two steps:\n",
    "\n",
    "1.   Create (or use existing) a AKS cluster for serving the model\n",
    "\n",
    "2.   Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "env.inferencing_stack_version='latest'\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "deployed_model = \"obj_segmentation_model\" # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "service_name = 'objservice2'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nums = [\"00001\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    img.show()\n",
    "    \n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "inputs = json.dumps(image_np_list)\n",
    "\n",
    "resp = service.run(inputs)\n",
    "\n",
    "predicts = resp[\"predicts\"]\n",
    "p_str = json.dumps(predicts)\n",
    "\n",
    "p_obj = json.loads(p_str)\n",
    "\n",
    "# put the model in evaluation mode\n",
    "for image_data in p_obj:\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a  function to call the url end point\n",
    "\n",
    "Go to EndPonts section of your azure machine learning workspace, you will find the service you deployed. Click the service name, then click \"consume\", you will see the restful end point (the uri) and api key of your service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def service_infer(url, body, api_key):\n",
    "    headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "\n",
    "        result = response.read()\n",
    "        return result\n",
    "\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The deployed service endpoints will be shown in workspace's EndPoints section. You may find the REST url and apiKey in  the consume part of the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'http://13.90.195.160:80/api/v1/service/objservice/score'\n",
    "api_key = 'hSySjdRpcqXLXqVyd7V5e1oaRUEbXhpu'  # Replace this with the API key for the web service\n",
    "\n",
    "img_nums = [\"00001\",\"00002\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img.show()\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "inputs = json.dumps(image_np_list)\n",
    "\n",
    "body = str.encode(inputs)\n",
    "resp = service_infer(url, body, api_key)\n",
    "\n",
    "# predicts = resp[\"predicts\"]\n",
    "# p_str = json.dumps(predicts)\n",
    "\n",
    "p_obj = json.loads(resp)\n",
    "\n",
    "# put the model in evaluation mode\n",
    "for image_data in p_obj[\"predicts\"]:\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to local computer as a Local Docker Web Service\n",
    "\n",
    "Make sure you have Docker installed and running.\n",
    "\n",
    "Note that the service creation can take few minutes.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The Docker image runs as a Linux container. If you are running Docker for Windows, you need to ensure the Linux Engine is running.\n",
    "PowerShell command to switch to Linux engine is\n",
    "\n",
    "<pre>\n",
    "& 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchLinuxEngine\n",
    "</pre>\n",
    "\n",
    "For more details, please see [this notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "# This is optional, if not provided Docker will choose a random unused port.\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "deployed_model = \"obj_segmentation_model\" # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "local_service = Model.deploy(ws, \"localtest\", [model], inference_config, deployment_config)\n",
    "\n",
    "local_service.wait_for_deployment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Local service port: {}'.format(local_service.port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status and Get Container Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test local deployment\n",
    "\n",
    "You can use local_service.run to test your deployment as shown in  case of deployment to Kubernetes Cluster. You can use end point to test your inference service as well.  In this local deployment, the endpoint is http://localhost:{port}/score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
